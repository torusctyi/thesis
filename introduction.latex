\chapter{Introduction}
\section{Preliminaries}



\subsection{Functional Data Analysis}
Functional data analysis (FDA) is a field of statistics where it is assumed that the data observed at a given set of independent observation times (or coordinates etc.) represent noisy observations of some underlying function.\cite{ramsay2005functional}

The approach taken here is to assume that an unknown differential equation can adquately -  though not necessarily exactly -  describe the process producing the data.


\subsubsection{Specification of Function Spaces}

The functions in question are generally all assumed to be members of some  countably infinite dimensional vector space, such as  the set of all functions $f(\cdot)$ such that $\int_0^T  |f''(t)|^2 dt < \infty$ over some interval $[0, T].$

This assumption implies that any given function can be represented as a countably infinite combination of basis elements, which are themselves functions. This means for a chosen set of basis elements $\{\phi_1(t), \phi_2(t), \dots\}$ and any given function $f(t),$ there is a set of coefficients $\{a_1, a_2, \dots \}$ such that:

\[
   f(t) = a_1\phi_1(t) + a_2\phi_2(t) + \dots
\]

Functional Data Analysis  can thus be regarded as a generalisation of multivariate statistics where the number of dimensions is potentially infinite.


Substantial complications are introduced into the statistical analysis because functions are generally much richer objects than real numbers or vectors. A function will generally have a different value for each input value, and the number of non-integer numbers on any interval - and hence potential inputs - is infinite. Functions cannot be trivially represented on paper or in computer memory in a similar fashion as real numbers or vectors.

For the purposes of this thesis, it is assumed that the functions in question are continous mappings.

In practice one attempts to resolve the problem of potential infite-dimensionality by finding or otherwise constructing a discrete problem that resembles the functional problem, and then solving this approximate problem instead.

Statistical problems that involve differential equations are particularly difficult. A naive approach is to force the practitioner to solve the ordinary differential equation (ODE) numerically again every time it is desired to evaluate the goodness of fit for a given choice of parameters. For these situations, it is necesary by definition to use numerical analytic techniques to construct a proxy problem that resembles the original problem sufficiently well and that is  sufficiently easy to tackle computationally.
 
For example, consider the problem of parametric estimation for a stochastic differential equation (SDE) of the form

\[
    dX = f(X; \theta)dt + \sigma dW.
\]

Here  $X(t)$ is the stochastic process being modelled, $f(\cdot;\theta)$ is a known function with a parameter $\theta$ to be estimated, $\sigma$ is a volatility parameter, and $W(t)$ is a standard Brownian motion.

This SDE is equivalent to asserting for any time $t$ and increment $h$ that

\[
   X(t + h) = X(t) + \int_t^{t + h}f(X(s); \theta)ds + \sigma[W(t+h) - W(t)].
\]

Suppose there are  observations $X_1, X_2, \dots, X_N$ of $X(t)$ at evenly spaced times, and that $h$ is the distance between the time points. The integral formulation of the SDE  suggests that if $h$ is small enough, then

\[
   X_{k+1} \approx X_k + f(X_k; \theta) + \sigma h Z_{k+1}.
\]

The $Z_k$ here are i.i.d standard Normal random variables. This is known as the \emph{Euler-Maruyama Approximation}. 

Instead of attempting to estimate parameters for the SDE, we can fit parameters for a non-linear AR(1) process that acts as a proxy problem for the original SDE. This is a much more tractable problem than the original SDE.

In FDA, the assumption is usually made that all the functions can be represented as a linear combination from some choosen \emph{finite} set of basis functions. Rather than discretise the differential operator as in the above example, the space of functions is discretised instead. 

A differential equation (or a similar problem) over some finite dimesional space of functions with $n$ dimensions can  be represented as a problem over the Euclidean space $R^n,$ this is a discrete problem.

The modelling process for functional data as described in Figure \ref{fig:statModellingProcess} can be more complex than standard statistical problems. 

\paragraph{Formulate a Model:}  As is the case for any statistical problem, the first step is to formulate a model. In the context of FDA, this often entails an ODE model. \cite{ramsay2009functional, ramsay2005functional} One must be certain that the model at used is sufficiently broad or well-specified to be able to actually capture the phenomona under investigation.



\paragraph{Construct a Discretised Model that Approximates the Original Model:} Unless the statistical model is trivial, the next step is to construct a proxy model. This generally requires ideas from Numerical Analysis.

\paragraph{Conduct Statistical Analysis Using the Discretised Model:} While the discretised model tends to be simpler than the original model, this task is not necesarily trivial. For FDA problems, R packages such as \texttt{FDA}\footnote{Discussed in Section \ref{sec:intro_fda_package}} and \texttt{Data2LD}\footnote{Discussed in Section \ref{sec:intro_data2ld}} that are designed to conduct such analysis are complex.

\paragraph{Check the Approximation Error in Discretised Model:} If the discretised model is too poor an approximation, then the results of any statistical analysis conducted could be substantially biased as a result of the  approximation error introduced, even if the original model were perfectly sound. If the orginal model is biased, then the approximate one might be even more so. 

Therefore, one should consider conducting post hoc checks. For example, running the analysis again with an alternative approximate model and comparing the results with the original model.  If both agree, it is evidence the approximate models are both reasonably accurate. 

In the context of FDA, this generally entails  increasing the number of basis functions so that the associated approximation error is smaller. 

For example, suppose that one were attempting to estimate the parameters of an ODE by means of least squares, and one was using a finite difference solver to compute the fitted values, and hence to determine the goodness-of-fit. 

Once the fitting alogrithm had converged, one might run the solver again with a smaller stepsize (or more basis functions) and the same parameters and check if this has made a substantial change in the the sum of squared residuals or whatever goodness-of-fit statistic is of interest.

Suppose that there has been a substantial change as a result of the stepsize reduction. One  would have to consider running the entire fitting procedure again starting from the previously computed parameter estimate, except with the smaller stepsize. If reducing the stepsize a second time doesn't produce a substantial change  in the goodness-of-fit statistic, one can be confident that no further reductions in the stepsize are necessary.

This procedure can be automated. The Implicit Filtering alogrithm discussed in Section \ref{sec:implicit_filtering}  is an example of such automation. On each iteration, Implicit Filtering computes an approximate gradient using finite differnences and uses this to perform optimisation.\cite{kelly2002filtering} If the Implicit Filtering alogorithm cannot produce a decrease in the objective function, or it cannot be certain that the true gradient isn't in fact zero, it reduces the stepsize. The algorithm terminates when the change in the objective function between changes in the stepsize has fallen below a chosen tolerance level.

If the fitting method used is slow however, then these such approaches can potentially be very slow  due to the need to solve the same problem over and over again at increasing levels of precision. 

Fortunately, Functional Data Analysis does not always require the recomputation of the curve in such a fashion whenever the parameters are changed. Instead of being implicitly represented as solutions of an ODE, functions are explicitly represented as elements in some finite dimesional vector space. As shall be seen, the objective function is generally a mapping from some vector space $R^n$ to $R$ that can often be evaluated reasonably easily, or at least more easily than having to run an ODE solver.

\paragraph{Check If Results of Statistical Analysis Are Consistent With Discretised Model.} In the previous step, one checked that the approximate model was actually acting as a proxy for the original model. One must then check that the statistical analysis conducted using the approximate model is valid in its own right. For example, it will be seen throughout this thesis that many statistical problems involving functions can be approximated by non-linear regression models. These constructed non-linear regression models should be checked for statistical validity.


\begin{figure}
   \centering
   \includegraphics[width = 13cm]{/home/padraig/programming/R/latex/chapter_2/modelling_2.pdf}
   \caption{Statistcal Modelling Process For Functions}
   \label{fig:statModellingProcess}
\end{figure}


\newpage

\section{Penalised Regression} \label{sec:pen_regression}

Suppose we have $N$ noisy observations $y_i$ at times $t_i$ from some function $f(t),$ and we wish to estimate $f(t),$ from the data. A naive approach would be to  estimate $f(t)$ by minimising a least squares criterion:

\[
   SSE(f) = \sum_{i = 1}^N [y_i - f(t_i)]^2
\]

Here, $SSE(\cdot)$ is a function that assigns a real number to every real-valued function that is defined for all the $t_i.$

There is an obvious problem with this criterion - it does not have a unique minimiser. Any function $g(t)$ such that $g(t_i) = y_i$ is will minimise $SSE(\cdot).$ There are an infinite number of degrees of freedom, but only a finite number of observations.

To ensure uniqueness, it is necessary to  impose a further condition to discriminate between different candidates, a way to choose between different functions that interpolate a given set of points.


\subsection{Smoothing Splines}
One potential criterion is to introduce a second order penalty. If two functions fit the observed data equally well, the more regular or less "wiggly" function is chosen. There are several ways of translating this intuition into a formal fitting procedure. 

A common choice is to measure the degree of irregularity by using the integral of the second derivative over a chosen interval $[0,T].$ The upper limit $T$ should be choosen to  allow for all observation times to be included.

\[
    \int_0^T |f''(t)|^2 dt.
\]


For a given set of points, the  smooth interpolating curve that minimises the  energy integral above is given by an interpolating cubic spline.

Choosing the most regular interpolating curve is not necessarily a very good estimation strategy however because it strongly prioritises goodness-of-fit above all other considerations. If the data is noisy, there is a risk of overfitting and poor predictive power. There is a trade-off between bias and variance. 

In practice, a joint estimation strategy is pursued that attempts to find a good balance between fidelity to the observed data and reasonably regular behavior. This involves minimising the following penalised least squares criterion:

\[
    PENSSE(f; \lambda) = \sum_{i = 1}^N (y_i - f(t_i))^2 + \lambda \int_0^T |f''(t)|^2 dt
\]

The $\lambda$ term dictates the trade-off between fidelity to the data and regularity. 

Suppose there were a candidate function $g(t),$ then by taking the cubic spline such that its value at $t_i$ is equal to $g(t_i),$ we can produce a curve $s(t)$ that has the same least-squares error as $g(t),$ but with $\int [s''(t)]^2 dt \leq \int [g''(t)]^2 dt.$ Thus, the curve that minimises $PENSSE$ can be assumed to be a cubic spline.


To find the minimiser of $PENSSE(\cdot; \lambda)$ first, assume that $f(t)$ can be represented as a linear combination of $K$ cubic spline functions $\phi_i(t)$ that can represent any cubic spline with knots at the $t_i.$ This implies that

\[
    f(t) = \sum_{i=1}^K c_i \phi_i(t).
\]

Note that it is only required that the set of basis splines only possess enough resolution to represent the function that minimises $PENSSE$, it is not required that this set of splines is minimal. 

Let the design matrix $\mathbf{\Phi}$ be defined by $\mathbf{\Phi}_{ij} = \phi_i(t_j),$ where $i$ indexes the basis functions and $j$ indexes the observations. Let the weight matrix $\mathbf{R}$ be defined by $\mathbf{R}_{ij} = \int_0^T \phi_i''(t)\phi_j''(t)dt.$ Then $PENSSE$ can be written in terms of the vector of coefficients $\mathbf{c}$ and observations $\mathbf{y}$ as:

\[
   PENSSE(\mathbf{c}; \lambda) = \|\mathbf{y} - \mathbf{\Phi c}\|^2+ \lambda \mathbf{c'Rc}
\]

The problem has been discretised into one on $R^K.$

The optimal value of $\mathbf{c}$ is given by

\[
  \mathbf{\hat{c}} = (\mathbf{\Phi'\Phi} + \lambda \mathbf{R})^{-1}\mathbf{\Phi'y}
\]

This is an exact solution to the original problem because the span of the $\{\phi_i(t)\}$ contains the function that minimises $PENSSE.$ The coefficient vector $\mathbf{\hat{c}}$ is the set of  coordinates of the optimal function within this finite-dimensional vector space. 




\subsection{Piecewise Trigonometric Interpolation}
Consider a more difficult penalised regression problem:

\[
    PENSSE(f; \lambda) = \sum_{i = 1}^N (y_i - f(t_i))^2 + \lambda \int_0^T |f''(t) - f(t)|^2 dt
\]

The penalty  $f''(t)$ has been replaced with a penalty $f''(t) - f(t).$
$PENSSE$ can be minimised in this case taking by a piecewise function consisting of linear combinations of $\sin(t)$ and $\cos(t)$ over each interval, and matching them together. 

Note that a function of the form

\[ 
   a_0 + a_1\cos(t) + b_1\sin(t) + a_2\cos(2t) + b_2\sin(2t) + \dots
\]

can be written as a polynomial in $e^{it}$ and $e^{-it}.$ For this reason, such a piecewise trignometric function can also be referred to as a piecewise trignometric polynomial or a piecewise trignometric spline.\cite{schumaker2007spline}

As can be seen in Figure~\ref{fig:pieceTrigCurve}, a piecewise trignometric polynomial of second degree  generally fails to be smooth at the boundary points, and thus  has a kinked appearance. For the purposes of statistical modelling, it is strongly desireable to impose the additional constraint that $f(t)$ must be everywhere differentiable. This cannot be achieved for a pieceswise basis formed from the functions $\{\sin(t), \cos(t)\}$ because there are only two free parameters on each segement and they are needed to ensure continuity.


\begin{figure}
   \centering
   \includegraphics[width = 13cm]{sin.pdf}
   \caption{Plot of a  Piecewise Trigonometic Curve. Note the kinks between segments.}
   \label{fig:pieceTrigCurve}
\end{figure}

\newpage
\section{Finte Dimensionalisation: the General Case} \label{sec:finite_dimension_general}

To find an exact solution to the two problems in Section \ref{sec:pen_regression}, it was necessary to construct a finite dimensional function space that contained the minimal function. However it is not guaranteed that this is always possible. In practice, one would hope that the optimal function can be approximated sufficiently well by taking a linear combination from some choosen set of functions. Spline bases tend to be a reliable workhorse that are effectively the default choice. They provide a good balance between being well behaved as objects for regression and having good approximating power. 

For comparison, Chebyshev Polynomials can often provide better approximation power for a given number of basis functions.\cite{boyd2001chebyshev} Unfortunately, it was found that they can be poorly behaved statistically because they consist of high order polynomials that are difficult to fit to data. 

Functional Data Analysis thus consists of the following steps, illustrated in Figures  \ref{fig:fda_modelling} and \ref{fig:fda_modelling_cyclical}:

\begin{enumerate}
 \item Formulate a model for $f(t).$ Usually, this takes the form of a penalised regression model, where $f(t)$ is defined as the function that minimises some kind of penalised error
\item Assume that $f(t)$ can be written as a finite combination of chosen basis functions. In practice a finte basis can only ever approximate $f(t),$ so it is imporant to ensure the basis is large enough to  approximate the optimal $f(t)$ sufficently well. The  function $f(t)$ can thus be written:

\begin{align*}
f(t) &= \sum_{i=1}^Kc_i\phi_i(t) \\
     &= [c_1, \dots, c_K]'[\phi_1(t), \dots, \phi_K(t)]\\
     &= \mathbf{c}^\top\boldsymbol{\phi}(t)
\end{align*}

Note that $f(t)$ is now defined by the coefficent vector $\mathbf{c}.$

\item Formulate the model in terms of the coefficient vector $\mathbf{c}.$ A statistical problem over some given functional space has been transformed into a statistical problem over $R^K.$

\end{enumerate}

For every valid choice of $\mathbf{c},$ a statistic that measures the  goodness of fit to  the data can be  computed. One desires the value of $\mathbf{c}$ that maximises the goodness-of-fit statistic under considetion.  The problem of finding the coefficients $\mathbf{c}$ can thus be thought of as being a non-linear regression problem since $\mathbf{c}$ is finite-dimensional.

Besides formulating an FDA model, one needs to consider the questions of constructing a finite dimensional approximation and then solving the associated non-linear regression. The situation is sketched in Figure \ref{fig:fda_modelling_cyclical}.

\begin{figure}
   \centering
   \includegraphics[width = 13cm]{/home/padraig/programming/R/latex/chapter_2/fda_modelling.pdf}
   \caption{Statistical Modelling Process For Functional Data Analysis}
   \label{fig:fda_modelling}
\end{figure}


\begin{figure}
   \centering
   \includegraphics[width = 13cm]{/home/padraig/programming/R/latex/chapter_2/fda_framework.pdf}
   \caption{Elements of Functional Data Analysis}
   \label{fig:fda_modelling_cyclical}
\end{figure}
\subsection{FDA With A Quadratic Basis} \label{sec:fda_quadratic_basis}
As carried out in \cite{boyd2001chebyshev}, we will immediately provide an example with a very small basis to illustrate these steps. Consider the following penalised regression problem:

\[
    PENSSE(f; \lambda) = \sum_{i = 1}^N [y_i - f(t_i)]^2 + \lambda \int_0^1 |t^2f'' - 0.5f|^2 dt
\]

The differential equation associated with the penalty term is known as an Euler's Equation. The solution is given by $f(t) = a t^{r_1} + b t^{r_2},$ where $r_1$ and $r_2$ are the roots of the quadratic equation $r^2 - r - 0.5 = 0.$ Thus, $r_1 \approx -0.36$ and $r_2 \approx 1.36.$

For the sake of illustration it will be assumed that that $f(t)$ can be written as a quadratic - a linear combination of the basis functions $\{1,t,t^2\}$:

\[
   f(t) = at^2 + bt + c
\]

Then: 

\begin{align*}
     \int_0^1 |t^2f''- 0.5f| dt &= \int_0^1 \left\lvert at^2 - \frac{1}{2}(at^2 + bt + c)\right\rvert^2dt \\
                                &= \int_0^1 \left\lvert \frac{1}{2}\left( at^2 - bt - c\right)\right\rvert^2dt \\
                                &= \frac{1}{4}\int_0^1 |at^2 - bt - c|^2 dt \\
                                &= \frac{1}{4}[a\ -b\ -c ]' \mathbf{H} [a\ -b\ -c ]\\
                                &= \frac{1}{4}[a\ b\ c]' (\mathbf{A'HA}) [a\ b\ c] \\
                                &= [a\ b\ c]' \mathbf{K} [a\ b\ c] 
\end{align*}
                     %     
Here $\mathbf{K} = \frac{1}{4}\mathbf{A'HA},$ the elements of the matrix $\mathbf{H}$ are defined by $\mathbf{H}_{ij} = \int_0^1 t^i t^j dt = 1/(i + j + 1),$  and elements of the matrix $\mathbf{A}$ are given by:

\[
\mathbf{A}= \left[\begin{matrix}1&0&0\\0&-1&0\\0&0&-1\end{matrix}\right]
\]

Thus, the penalised error is given by:

\begin{equation} \label{eqn:pen_quad}
   PENSSE(a,b,c; \lambda) = \sum_{i = 1}^N (y_i - at_i^2  - bt_i - c)^2   + \lambda [a\ b\ c]' \mathbf{K} [a\ b\ c] 
\end{equation}


We have now gone from a problem specified in terms of functions, to a penalised least squares problem in the three coefficents $a,b$ and $c.$ The quality of this approximate model as $\lambda$ gets larger and larger depends on how well the functions $t^{-0.36}$ and $t^{1.36}$ can be respectively approximated by  quadratics over the interval $[0,1].$ 

To illustrate this example further, the method was fitted to simulated data. A solution to the ODE $t^2f'' - f = 0$ was generated over the interval $[0,1],$ samples were taken at various points before being corrupted by Gaussian noise. The quadratic that minimised (\ref{eqn:pen_quad}) with $\lambda = 100$ was then found. For comparison, the data was also fitted to a quadratic using ordinary least squares. The original function $f(t),$ the perturbed data, and the two fitted functions are all shown in Figure \ref{fig:quad_model} 

It's already been noted that the quality of the model depends partially on how well $f(t)$ can ever   be approximated by a quadratic over $[0,1].$ Therefore, the quadratic $q(t)$ that minimises $\int_0^1|f(t) - q(t)|dt$ was found numerically  and  also plotted in Figure \ref{fig:quad_model}.

Figure \ref{fig:quad_model} suggests that $f(t)$ can be approximated reasonably well by quadratics  so long as one stays away from the point $t =  0.$  The ODE $t^2f'' - f = 0$ behaves degenerately at  the origin. When $t=0,$ the ODE has a singular point, the term in  front of $f''$ becomes zero so that the  ODE reduces to $(0)^2f'' - f = 0.$ Additionally, it is always the case that the second derivative diverges to infinity at $0$ if $f(t)$ is of the form $a t^{-0.36} + b t^{1.36}$. As a result of  both the singular point and infinite curvature at $t=0,$ polynomial approximation is theoreticallyection 1.3 developed FDA algorithms for penalised fitting from scratch. However the FDA package in R.... predicted to be execptionally tricky around this point.\cite{isaacson2012analysis, teschl2012ordinary}

Comparing the two fits in Figure \ref{fig:quad_model}, it is fair to argue that the penalised regression model captures the shape of $f(t)$ better than ordinary least squares away from $t=0.$ Both models seem to have similar predictive power on average. The penalised fit is being heavily influenced by the singularity  at $t=0$ and probably would have performed better if a more robust loss function than least squares were used.


 
\begin{figure}
\centering
\includegraphics[width = 9cm]{quad_model.pdf}
\caption{Performing FDA with the differential operator $Lf = t^2f'' - 0.5f$ and the basis set $\{1, t, t^2\}.$} \label{fig:quad_model}
\end{figure}



\newpage

\section{The \texttt{FDA} Package} \label{sec:intro_fda_package}
Section 1.3 developed FDA algorithms for penalised fitting from scratch. However the \texttt{FDA} package in R  was developed\cite{fdapackage,ramsay2009functional} to  tackle penalised problems of the form:

\begin{equation}
PENSSE(f) = \sum_{i=1}^N[y_i - f(t_i)]^2 + \lambda \int |Lf(t)|^2dt
\end{equation}

Here $Lf$ is a parameterised linear differential operator of the form $\sum_{j=0}^n \beta_jD^j$ where the $\beta_j$ are constants. The result of fitting the differential operator $Lf = f - \omega^2f^{(4)}$ with $\omega = 0.65$ is shown in Figure \ref{fig:fda_mela_smooth}. This operator was choosen because a penalty of the form $f - \omega^2f^{(4)}$ ignores functions of the form $c_1 + c_2t + c_3\sin(\omega t) + c_4 \cos(\omega t).$

The \texttt{FDA} packages is not as powerful as the \texttt{Data2LD} package, which will be introuduced in Section \ref{sec:intro_data2ld}. It has the advantage of simplicity and ease of use, and is used throughout this thesis to fit FDA models unless $\texttt{Data2LD}$ is essential. A deficiency of the \texttt{FDA} package is that it provides no guidence on the best choice of  the parameters $\beta_i$ nor the smoothing parameter $\lambda.$\footnote{The \texttt{FDA} package has a command called \texttt{lambda2gcv} whose documentation claims it `[finds] the smoothing parameter that minimizes GCV' \cite{fdapackage}. Inspection of the code for this function shows that it only performs a fit based on the value of $\lambda$ passed and then reports the GCV. Incorrect or unclear documentation is unfortunately not an uncommon problem with FDA codes.}

\begin{figure}
\centering
\includegraphics[height=11cm]{mela_standard_plot.pdf}
\caption{Using the \texttt{FDA} package to smooth the melanoma data with the differential operator $Lf = f - \omega^2f^{(4)}.$}
\label{fig:fda_mela_smooth}
\end{figure}

\newpage\clearpage

\section{The \texttt{Data2LD} Package} \label{sec:intro_data2ld}

The Data2LD package is an R package intended to perform smoothing using general linear differential operators with a forcing function, that is, ODEs of the form:

\begin{equation}
    \sum \beta_i(t)D^i f(t) = u(t)
\end{equation}

The $\beta_i(t)$ are paremeter functions for the linear differential opertor on the lefthand side, and $u(t)$ is a forcing function. 

More generally, \texttt{Data2LD} can model a system of inhomogenous linear differntial equations:

\begin{equation}\label{fig:ode_system}
   \mathbf{y}(t)' + \mathbf{B}(t)\mathbf{y} = \mathbf{u}(t)
\end{equation}

Each element of $\mathbf{B}(t)$ is a time-varying linear parameter function of the the form $\beta_{ij}(t)$ and each element of $\mathbf{u}(t)$ denotes the forcing function applied to the $i$th equation.

A further advantage of \texttt{Data2LD} over the \texttt{FDA} package is that not only can it smooth ODEs with functional parameters, but it can estimate the associated parameters even if they are functions.

While \texttt{Data2LD} can estimate parameters for the differential operator, it does not provide a means for finding the optimal smoothing parameter.\footnote{For \texttt{Data2LD}, the smoothing parameter is written in terms of $\rho = \lambda/(1 + \lambda).$}

\section{Modelling the Reflux Data: A Parametric Approach vs \texttt{Data2LD}} \label{sec:reflux_parametric_vs_data2ld}

The Reflux data, plotted in Figure \ref{fig:refluxPlot}, describes the output of an oil refining system. A given fraction of oil is being distilled into a specific tray, at which point it flows out through a valve. At a given time, the valve is switched off, and distillate starts to accumulate in the tray \cite{ramsay2005functional}. The Reflux data was taken from the \texttt{Data2LD} package used for FDA, which will be discussed in more detail later. The authors of the \texttt{Data2LD} package model the data using the following ODE:

\begin{figure}
   \centering
   \includegraphics[width = 10cm]{/home/padraig/programming/R/latex/chapter_2/tray.pdf}
   \includegraphics[width = 10cm]{/home/padraig/programming/R/latex/chapter_2/valve.pdf}
   \caption{Reflux Data}
   \label{fig:refluxPlot}
\end{figure}

\begin{equation}\label{eqn:reflux_ode}
\begin{cases} 
      y'(t) = -\beta y(t) & t\leq t_0 \\
      y'(t) = -\beta y(t)  + u_0 &  t\geq t_0 \\
      y(0) = 0 \\
   \end{cases}
\end{equation}

Up until the point $t_0,$ the function satisfies the ODE $y' = -\beta y.$ At the breakpoint, a constant forcing function $u_0$ is turned on to model the valve being switched off, so that the ODE then becomes $y' = -\beta y  + u_0.$

This ODE admits an exact solution. Letting $\gamma = u_0/\beta$ and $C$ be an arbitray constant, then the solution is given by

\[ y(t) = \begin{cases} 
      0 & t \leq t_0 \\
      \gamma + Ce^{-\beta (t-t_0)}  & t\geq t_0 \\
   \end{cases} 
\]

Without loss of generality the  exponential term $Ce^{-\beta (t - t_0)}$ can be replaced with one that is of the form $ Ce^{-\beta t}.$ This is the case because  $Ce^{-\beta (t - t_0)} = Ce^{-\beta t}e^{-\beta t_0} = [Ce^{-\beta t_0}e^{-\beta t}],$ the $e^{-\beta t_0}$ term is thus absorbed into the  constant term.

In order to ensure that $y(t)$ is continous at $t_0$ and monotone increasing, we require that $\gamma + C =0 $ and that $\beta > 0$


\subsection{Parametric Approach} \label{sec:reflux_parametric_approach}
It turns out that the constraint $C = -\gamma$ is unsuitable from the point of view of numerical parameter estimation because R's \texttt{nls} command reports errors when this constraint is imposed.

However, if we allow $t_0$ to vary, we can allow $C$ to assume any negative value while preserving monotonicity and continuity.

Assume that $y(t)$ is instead given by:

\[
   \tilde{y}(t) = \max(0, \gamma + Ce^{-\beta (t - t_0)})
\]


The function $\tilde{y}(t)$ satisfies the  same ODE and initial conditions as $y(t)$ except that the change point  $t_0$ is shifted to $t_0'$ defined by:

\[
   t_0'  = \max\left(t_0, t_0 - \frac{1}{\beta}\ln\left(\frac{ - \gamma}{C}\right) \right)
\]

The function $\tilde{y}(t)$ is a combination of simpler functions, joined together using the maximum operator instead of the addition operator, see Figure ~\ref{fig:constituentMaxFunctions}.

\begin{figure}
   \includegraphics[width=1\textwidth]{piecewise.pdf}
   \caption{Plot of $\tilde{y}(t)$ and its constituent functions}
   \label{fig:constituentMaxFunctions}
\end{figure}

\newpage
\subsubsection{Parametric Fitting}

Instead of approximately solving an associated problem as discussed in Section \ref{sec:finite_dimension_general}, a  purely parametric approach to fitting  the ODE (\ref{eqn:reflux_ode}) will be employed. The question of modelling the Reflux data using FDA will be discussed in a later chapter.

 We assume that the breakpoint $t_0$ is known in advance. Then our model for $y(t)$

\begin{equation} \label{eqn:reflux_model}
  y(t) = \begin{cases} 
      0 & t \leq t_0 \\
      \beta_0 + \beta_1e^{\beta_2t}   & t\geq t_0 \\
   \end{cases} 
\end{equation}

Note that this function might not be well defined at $t_0,$ we will adress the question of matching later on.
We must estimate the three unknown coefficients $\beta_0, \beta_1, \beta_2.$

\paragraph{Estimating $\beta_0$ from the data:}

Figure \ref{fig:refluxPlot} suggests that $\beta_2 < 0,$ and $\beta_1 < 0,$ under this assumption, we have that:

\[
   \lim_{t \rightarrow \infty} y(t) = \beta_0
\]
Where the convergence happens monotonically from below

So an initial estimate for $\beta_0$ is given by $\hat{\beta}_0 = \max(y_i)$

\paragraph{Estimating $\beta_1$ and $\beta_2$  from $\beta_0$ and the data:}

For $t \geq t_0,$ the model in Equation \ref{eqn:reflux_model} can be rearranged so that:

\begin{equation} \label{eqn:reflux_rearranged_model}
   \log(\beta_0 - y(t)) = \log|\beta_1| + \beta_2t
\end{equation}


This equation is only valid so long as the left hand side is well defined however. It is  necessary to exclude the largest observed value of $y,$ because $\beta_0$ is estimated to be the largest obsevation at this point. If the largest value were included, there would be a term of the form $\log(0)$ in the rearranged model (\ref{eqn:reflux_rearranged_model}).

The values of  $\log|\beta_1|$ and $\beta_2$ can be estimated by  performing linear regression against $\log(\beta_0 - y(t)),$ with the largest value of $y$ observed excluded. It was assumed that $\beta_1 < 0,$ so $\hat{\beta_1}$ can be found from the estimate of $\log|\beta_1|.$  

 
\paragraph{Simulataneous Estimation of Parameters:}

Now that we have reasonable estimates for $\beta_0, \beta_1,$ and $\beta_2,$ we can use non linear regression to estimate all three jointly.

\paragraph{Matching:}

For $t < t_0,$ it is estimated that $\hat{y}(t) = 0.$ For $t \geq t_0.$ the estimate is given by $\hat{y}(t) = \hat{\beta_0} + \hat{\beta_1}e^{\hat{\beta_2}t}.$  There are distinct  estimates for $y(t)$ at $t \leq t_0$ and $t \geq t_0,$  which  do not necessarily agree at $t = t_0.$ This is the case for the estimates produce here since $\hat{y}(t_0) = 0.029.$

To stitch the two functions together,  let $\hat{y}(t) = \max(0, \hat{\beta_0} + \hat{\beta_1}e^{\hat{\beta_2}t}).$ This is a continous function that entirely satisfies the orginal ODE, except for the precise location of the breakpoint.

The resulting fit is  presented in Figure \ref{fig:reflux_ode_fit}.

\paragraph{Breakpoint Estimation:} The value of $t_0$ used for the fit is given by $t_0 = 68.$ A statistical estimate of the breakpoint can be found from finding the point where $\hat{\beta}_0 + \hat{\beta}_1e^{\hat{\beta}_2t}$ is zero:

\[
   \hat{t}_0  = \left\lvert\frac{1}{\hat{\beta}_2}\log\left(-\frac{\hat{\beta}_0}{ \hat{\beta}_1}\right)\right\rvert
\]

Using this formula, it was estimated that ${t}_0 = 67.71.$ This new value will produce the same results as for $t_0 = 68$ because it doesn't change the set of  observation points  used to estimate $\beta_0, \beta_1,$ and $\beta_2.$

\subsubsection{Discussion}

The parametric approach taken to estimation here is somewhat \textit{ad hoc.} Instead of devising a formal estimation strategy in advance, the fitting approach evolved organically alongside the problems of solving the $ODE$ and fitting the data. Use was made of properties tied to the ODE model to compute estimates. While this has produced an effective fit, there are obvious concerns about generalising this approach to other ODEs. Futhermore, since the fitting model was devised by peeking at the data, it is not obvious that one can find a valid p-value for the fit without getting an entirely new set of data. 

This issue is difficult to resolve using purely parametric methods. It is often the case in Applied Mathematics that one can't fully investigate an ODE model until one has a rough grasp of its behaviour. It has been demonstrated that the  associated Statistical fitting problem inherits this tendency. 

\begin{figure}
\centering
\includegraphics[height=11cm]{nls_fit.pdf}
\caption{Fitting the Reflux data to the ODE model parametrically.}
\label{fig:reflux_ode_fit}
\end{figure}

\subsection{Fitting the Reflux Data with Data2LD}

While the parametric approach employed in Section \ref{sec:reflux_parametric_approach} requires a considerable amount of domain-specific knowledge,  the functional model can be more  generally employed. The FDA approach doesn't rely on individual features of the specific differential equation at hand,\footnote{The FDA approach does rely on more general features of course, such as whether or not the differential equation is linear.} and produces a similar fit to the Reflux data as the parametric approach.

The functional model asserts that 

\[
   y'(t) \approx -\beta y(t) + u(t)
\]

Where $y(\cdot)$ and $u(\cdot)$ are functions to be estimated, and $\beta$ is a single scalar parameter.

It is assumed that $u(t)$ is a step function of the form

\[
   u(t) = a\mathbb{I}_{[0, t_0)}(t) + b\mathbb{I}_{[t_0, \infty)}(t)
\]

As in the parametric case, the breakpoint $t_0$ is fixed in advance. It is further assumed that $y(t)$ can be expanded as a linear combination of B-Splines. The knots are duplicated at $t_0$ so that the first derivative at the breakpoint is discontinous. 

This model was fitted using the \texttt{Data2LD} package, and the results are plotted in Figure \ref{fig:reflux_fda_fit}. It can be seen that the fit is quite similar to the parametric one presented in Figure \ref{fig:reflux_ode_fit}. The main disadvantage of the FDA approach compared to the parametric one is that \texttt{Data2LD} can be complex and unintuitive to use.


\begin{figure}
   \centering
   \includegraphics[height=11cm]{fda_plot.pdf}
   \caption{Modelling the Reflux data using \texttt{Data2LD}.}
   \label{fig:reflux_fda_fit}
\end{figure}


\section[Rates Of Convergence]{Rates Of Convergence\footnote{There are slightly different definitions of convegrgence rates from text to text, but all capture the same basic meaning.  Refer to \cite{nocedalnumerical, chong2013introduction}.}}\label{sec:rates_of_convergence}

Throughout this thesis, it will sometimes be desireable to consider the rates of convergence of different fitting and estimation methods. For the purposes of this section, it is assumed that there is a vector-valued sequence $\mathbf{x}_0, \mathbf{x}_1,  \mathbf{x}_2, \dots$ that  converges to a value $\mathbf{x^*}.$ 

\paragraph{Linear Convergence:}A convergent sequence is said to \emph{converge linearly}\footnote{In \cite{nocedalnumerical}, the case $\mu = 0$ is considered to be a case of linear convergence as well. This definition makes it harder to sharply discriminate between linear and superlinear convergence.} to $\mathbf{x^*}$ (with convergence rate $\mu$) if there is a $0 < \mu <1$ such that:

\begin{equation}\label{eqn:linear_convergence}
   \lim_{n \rightarrow \infty}  \frac{\|\mathbf{x}_{n+1}- \mathbf{x}^*\|}{\|\mathbf{x}_{n}- \mathbf{x}^*\|} = \mu
\end{equation}

If a sequence $\mathbf{x}_n$ converges linearly with constant $\mu,$ then $\|\mathbf{x}_{n+1}- \mathbf{x}^*\| \approx \mu\|\mathbf{x}_{n}- \mathbf{x}^*\|$ for $n$ sufficiently large. A simple example of a linearly converging sequence is given by $1, \frac{1}{2}, \frac{1}{4}, \frac{1}{8},\dots$ If plotted on a log scale, the error terms $\|\mathbf{x}_{n+1}- \mathbf{x}^*\|$  will  tend to lie on a straight line. A linearly convergent sequence has the property that if the number of iterations is doubled, then the number of digits of  precision achieved is roughly doubled as well. 

\paragraph{Sublinear Convergence:}A sequence is said to converge  \emph{sublinearly} to $\mathbf{x^*}$ if:

\[
   \lim_{n \rightarrow \infty}  \frac{\|\mathbf{x}_{n+1}- \mathbf{x}^*\|}{\|\mathbf{x}_{n}- \mathbf{x}^*\|} = 1
\]

Sublinear convergence is very slow. Every reduction in the order of magnitude of the error achieved takes more iterations than the previous reduction. The ur-example of a sublinearly convergent sequence is the reciprocals of the natural numbers: $1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, \dots.$ 

\paragraph{Superlinear and Quadratic Convergence:}A sequence is said to converge superlinearly if:

\[
   \lim_{n \rightarrow \infty}  \frac{\|\mathbf{x}_{n+1}- \mathbf{x}^*\|}{\|\mathbf{x}_{n}- \mathbf{x}^*\|} = 0
\]

 A sequence is said to converge superlinearly with order $p$ if there exist positive constants $p > 1$ and $\mu>0$ such that:
\begin{equation}\label{eqn:linear_convergence}
   \lim_{n \rightarrow \infty}  \frac{\|\mathbf{x}_{n+1}- \mathbf{x}^*\|}{\|\mathbf{x}_{n}- \mathbf{x}^*\|^p} = \mu
\end{equation}

If $p=2$, the sequence is said to converge quadratically.

Note that there is no requirement that $\mu < 1$ in this case. If $p=2,$ the sequence is said to converge quadratically.Taking logs yields that $\log(\|\mathbf{x}_{n+1}- \mathbf{x}^*\|) \approx -p\log(\|\mathbf{x}_{n}- \mathbf{x}^*\|).$ For a linearly convergent sequence, the magnitude of the error declines exponentially and the number of digits of precision gained increases linearly with the number of iterations. But for a superlinearly convergent sequence, the \emph{order of magnitude of the error} declines exponentially and the number of digits of precision gained grows geometrically with the number of iterations. For a quadratically converging sequence, each iteration tends to roughly double the number of digits of precision. For example, if the error in the first iterate is approximately $0.1$, the next iterate will have error on the order of $10^{-2}$, the next again will have error on the order of $10^{-4},$ and so on. 
An  example of superlinear convergence is given by the sequence $x_n = 2^{-2^n}.$ 

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Convergence Class & Example               & Iterations until $<10^{-6}$        & Iterations until $<10^{-12}$  \\
\hline
Sublinear        & $x_n = \frac{1}{n}$    & $10^6 + 1$                         & $10^{12}$ + 1                \\
Linear           & $x_n = 2^{-n}$         & 20                                 & 40                            \\
Superlinear      & $x_n  = 2^{-2^{n}}$    & 5                                  & 6                             \\
\hline                                              
\end{tabular}    
\caption{Illustrating the different classes of convergence.}  
\end{table}


\paragraph{An Extended Definition of Convergence Rates:}The above appraoch to defining the rate of convergence can't handle every sequence however. For example, the  sequence $1,1, \frac{1}{2}, \frac{1}{2}, \frac{1}{4}, \frac{1}{4}, \dots$ does not converge linearly in the sense of (\ref{eqn:linear_convergence}). To cover these situations, a sequence is also said to converge linearly/sublinearly/superlinearly if there is an associated auxillary sequence $\epsilon_n$ such that $\|\mathbf{x}_{n+1}- \mathbf{x}^*\| \leq \epsilon_n$ for all $n \geq 0,$ and the sequence $\epsilon_n$ converges linearly/sublinearly/superlinearly to zero.\footnote{The simple definition presented here is known as \emph{Q-Convergence}, and the extended definition is known as \emph{R-Convergence} \cite{nocedalnumerical}.}


\paragraph{Linear Convergence and Iterated Mappings:}Nearly all estimation algorithms used in statistics start with an initial estimate $\theta_0$ and generate a sequence of estimates by $\mathbf{\theta}_{n+1} = \mathbf{\mathbf{M}}(\theta_n)$ for some mapping $\mathbf{M}(\cdot).$ The alogrithm is stopped when the generated sequence has converged within a tolerance of the limit $\theta^*.$ Examples include the Newton-Raphson Method, Fisher's Method of Scoring, Gradient Descent, the EM Algorithm,  Block Relaxation, and many imputation methods. As shall be seen, a statistically motiviated fitting alogorithm will nearly always converge linearly unless it has been specifically engineered so that $\mathbf{M}'(\theta^*) = 0.$

 Linear convergence is common for  convergent sequences defined by repeatedly applying a function $\mathbf{f}$ so that $\mathbf{x}_{n+1} = \mathbf{f}(\mathbf{x}_n).$ To see this, perform a Taylor expansion about the limit point $\mathbf{x^*}:$

\begin{align*}
\mathbf{f}(\mathbf{x}_{n}) &\approx \mathbf{f}(\mathbf{x^*}) + \mathbf{f'}(\mathbf{x^*})(\mathbf{x}_n - \mathbf{x^*}) \\
\mathbf{f}(\mathbf{x}_{n}) &\approx \mathbf{x^*} + \mathbf{f'}(\mathbf{x^*})(\mathbf{x}_n - \mathbf{x^*}) \\
\mathbf{f}(\mathbf{x}_{n}) - \mathbf{x^*} &\approx \mathbf{f'}(\mathbf{x^*})(\mathbf{x}_n - \mathbf{x^*}) \\
\mathbf{x}_{n+1}  - \mathbf{x^*} &\approx \mathbf{f'}(\mathbf{x^*})(\mathbf{x}_n - \mathbf{x^*}) \\
\end{align*}

Taking norms of both sides yields that:

\[
   \|\mathbf{x}_{n+1}  - \mathbf{x^*}\| \lesssim \|\mathbf{f'}(\mathbf{x^*}) \| \|\mathbf{x}_{n+1}  - \mathbf{x^*}\|
\]

The situation here is a little subtle because $\mathbf{f}$ is a multivariate function. The exact rate is of convergence is controlled by the norm of the Jacobian matrix $\mathbf{f}'(\mathbf{x})$ at $\mathbf{x^*}.$ So long as there is a matrix norm such that $\|\mathbf{f}'(\mathbf{x}^*)\| < 1$ the sequence will converge linearly at worst, though faster than linear convergence is potentially possible if $0$ is an eigenvalue of $\mathbf{f}'(\mathbf{x}^*).$\footnote{Consider for example the multivariate sequence defined by $(x_{n+1}, y_{n+1}) = (x_n^2, y_n/2).$  This convergence  towards zero is  superlinear  in the $x$ direction, but only linear in the $y$ direction. If $(x_0, y_0) = (0.5, 0),$ then the convergence will be superlinear. Usually however the $y$ component will be nonzero and will drag the convergence rate down to linear convergence.} If $\mathbf{f'}(\mathbf{x^*}) = \mathbf{0},$ the convergence will be superlinear.





\section{Overview of Appendices}

Appendix A contains an overview of the optimisation methods used throughout this thesis. It can be safely skipped if one is already familiar with the theory unconstrained optimisation up to Line Search Methods. Line Search methods can be thought of as a generalisation of Gradient Descent or the Newton-Raphson method where the size of the step taken (sometimes referred to as the \emph{Learning Rate}) varies on each iteration. The material in Appendix A is a prerequisite for Chapter 2 in particular.

Appendix B discusses the Implicit Filtering method. Implicit Filtering was found to be inadaquate for our purposes and hence does not play any positive or active role in this thesis. The material associated with this purely negative result is accordingly relegated to the appendices.






