

\chapter{Profiling Non-Linear Regression Models With the Parameter Cascade} \label{chap:nonlinear}

In previous chapters, the ODE models considered were linear. The work in this chapter generalises the results to non-linear ODEs and PDEs. We will discuss regression models that can be separated into an \emph{inner problem} where one attempts to find the best fit given a choice of parameters, and a \emph{middle problem} where one attempts to find the optimal choice of parameters. First a simple nonlinear regression model will be discussed as an example to introduce the ideas used in this chapter. Then, a Parameter Cascade method for fitting linear ODEs is devised. Finally, the Parameter Cascade approach will be used to fit PDEs. The approach used in this chapter here is related to an ODE fitting methodology developed by Ramsay et al \cite{ramsay2017dynamic, hooker2016collocinfer}. The authors attempt to fit a parameterised ODE $\mathbf{x}'(t) = \mathbf{f}(\mathbf{x}(t), t| \theta)$ using a penalised regression model of the form:

\[
    PENSSE(\mathbf{x}(t); \theta,\lambda) = \sum[\mathbf{y}_i - \mathbf{x}(t_i)]^2 + \lambda \int_0^T[\mathbf{x}'(t) - \mathbf{f}(\mathbf{x}(t), t| \theta)]^2dt
\]

\noindent The first term measures fit to the data, and the second measures fidelity to the ODE. This in turn defines a middle problem for $\theta\colon$

\[
   H(\theta) = \sum[\mathbf{y} - \hat{\mathbf{x}}(t_i|\theta, \lambda)]^2,
\]

\noindent where $\hat{\mathbf{x}}(t_i|\theta, \lambda)$ is the minimiser of $PENSSE(\mathbf{x}(t); \theta,\lambda)$ for fixed values of $\theta$ and $\lambda.$ It is assumed that $\mathbf{x}(t)$ can be expanded as a linear combination of basis functions, and the authors proceed to conduct a traditional Parameter Cascade.\footnote{Not \emph{entirely} traditional. To perform a fit, derivatives of $\mathbf{f}(\cdot)$ with respect to $\mathbf{\theta}$ and $\mathbf{x}$ are required. \texttt{CollocInfer} uses finite differencing to estimate these derivatives if no information is provided by the user.} A sophisticated R package called \texttt{CollocInfer} has been developed to implement the method in a similar fashion as \texttt{Data2LD}. \\

\noindent The approach used in this chapter is more in the spirit of symmetry methods used to solve differential equations as described in \cite{hydon2000symmetry}. The aim is to exploit structure in the specific dynamical system being modelled so that the problem of fitting the system is turned into a transformation model of some kind, which can then be fitted using a hierarchical approach. Our propsed approach can fit first-order linear PDEs with ease for example, because it exploits the fact that the solutions to these PDEs are constant along certain curves whose shape depend on the PDE at hand and the parameters.


\section{Fitting a Non-Linear Regression Model With the Parameter Cascade} \label{sec:param_cascade_nonlinear_regress}

Consider the following non-linear regression model where observed values $y_i$ are values observed at times $t_i$ :

\begin{equation}
   y_i = \alpha + \beta e^{\gamma t_i} + \epsilon_i.
   \label{eqn:nonlinear_model}
\end{equation}

\noindent This would be a straightforward linear regression problem if not for the unknown  $e^{\gamma t}$ term. If $\gamma$ were known, $\alpha$ and $\beta$ could be found through simple linear regression with the $e^{\gamma t_i}$ term acting as an independent variable predicting the $y_i$. This suggests the following regression strategy. Define a function $H(\gamma)$ to be the sum of squared errors from performing simple linear regression on the $y_i$ against $e^{\gamma t_i}.$ That is:

\[
   H(\gamma) = \min_{\alpha,\beta} \sum [y_i - \alpha -\beta e^{\gamma t_i}]^2
\]

\noindent This defines a middle problem, with the inner problem being that of minimising the simple linear regression problem given $\gamma.$ The non-linear model can be estimated by using Brent's method to fit the middle problem. This approach was applied to simulated data with $\alpha = 100, \beta = 4,$ and $\gamma = 1$, and the results can be seen in Figure \ref{fig:non_linear_model}. Things proceed as in the previous chapters, the  parameter $\gamma$ has been estimated with high precision, and the fit is of good quality.

\newpage
\begin{figure}[h]
\centering
\includegraphics[height=9cm]{exp_param_cascade_profile.pdf}
\includegraphics[height=9cm]{exp_param_cascade_fit.pdf}
\caption{Profile Plot and Fitted Curve}
\label{fig:non_linear_model}
\end{figure}

\newpage

\section{Fitting Linear Homogeneous ODEs Using the Parameter Cascade}

Recall a linear homogeneous ODE of order $n$ is given by:

\[
   \frac{d^ny}{dt^n} = \sum_{k=0}^{n-1} a_k(t; \theta)\frac{d^ky}{dt^k}.
\]

\noindent Under some mild technical conditions, the set of solutions to such an ODE is an $n$ dimensional vector space and has a unique solution for each set of initial conditions. It is often more convenient to work with ODEs in matrix form. Any homogeneous linear ODE can be represented in matrix form:

\[
   \frac{d\mathbf{y}}{dt} = \mathbf{A}(t; \theta)\mathbf{y}.
\]

\noindent For example, the ODE $y'' = - \omega^2y$ with the initial conditions $y(0) = y_0$ and $y'(0) = v_0$ can be represented as:

\[
  \frac{d}{dt} \left(\begin{matrix}y_1\\y_2\end{matrix}\right) = \left(\begin{matrix}0&1\\-\omega^2&0\end{matrix}\right)\left(\begin{matrix}y_1\\y_2\end{matrix}\right)
\]

\noindent with the initial condition $\mathbf{y}(0) = (y_0, v_0)'.$ A basis for the solution set of any linear ODE can be formed from the set of solutions associated with the initial conditions $\mathbf{y}(0) = \mathbf{e}_i,$ where  $\mathbf{e}_i$ denotes the $i$th basis vector. This suggests the following cascade algorithm: given a set of parameters, find the set of solutions $\{y_1(t|\theta), \dots, y_n(t|\theta)\},$ where $y_k(t|\theta)$ denotes the solution with the $k$th basis vector as an initial condition. Then perform regression to fit the $\{y_i(t|\theta)\}$ to the observed data. The inner problem then consists of fitting a weighted sum of the $\{y_i(t|\theta)\}$ to the observed data and reporting the associated error given a choice of parameters, while the middle problem consists of finding the set of parameters that minimises this associated error. \\

\noindent For a problem where the ODE can be solved explicitly, things proceed as in Section \ref{sec:param_cascade_nonlinear_regress}. Consider again the ODE $y'' - \omega^2y = 0$. The solutions generated by the initial conditions $(1,0)$ and $(0,1)$ is given by $\{A\cos{\omega t} + B\sin{\omega t} |A,B \in \mathbb{R}\}.$ So the middle (least squares) criterion is given by:

\[
    H(\omega) = \min_{a,b} \sum[y_i - a\cos{\omega t_i} - b\sin{\omega t_i}]^2
\]

\noindent Finding the optimal $a$ and $b$ given $\omega$ is an inner problem that can be solved using least squares regression as before. In fact, such a problem has already been encountered: the  nonlinear model given in Equation \ref{eqn:nonlinear_model} is associated with the ODE $y'' - \gamma y' = 0.$ For ODE problems that cannot be explicitly solved, the trajectories $y_n(t|\theta)$ must instead be found by a numerical solver for each choice of $\theta.$ The inner problem then consists of linearly regressing the computed solutions against the observed data. To illustrate the method, it was applied to the following ODE with $\alpha = -0.3$ and $\beta = -1.0:$ 

\begin{equation}
   y''(t) =  \alpha \sqrt{t} y(t) + \beta \sin(2t) y'(t)
  \label{eqn:ode_model}
\end{equation}

\noindent To minimise the middle problem, the Nelder-Mead method was used as Brent's method was was unsuitable because of the awkward topography. The results can be seen in Figure \ref{fig:ode_plot}. The objective function has a complex topography, the method can fail to converge to the correct minimum if the sample size is too small or the variance is too high. In this case the parameters were sucessfully estimated and a good fit was producd.\\

\begin{figure} 
    \centering
    \includegraphics[height=9cm]{ode_param_cascade_fit.pdf}
    \includegraphics[height=9cm]{ode_param_cascade_contour.pdf}
    \caption{Plot of fit to simulated data,  and contour plot of SSE against $\alpha$ and $\beta.$ Blue dot is true parameter values, red is estimated parameter values.}
    \label{fig:ode_plot}
\end{figure}

\noindent The advantage of the Parameter Cascade here over a generic optimisation routine is that it is noticeably faster than trying to optimise everything in one go. The linear regression steps mean that the ODE needs to solved numerically fewer times, so that the algorithm runs approximately $30\%$ faster for the ODE in Equation \ref{eqn:ode_model}. However, the \verb|nls| command is faster than the Parameter Cascade for Equation \ref{eqn:nonlinear_model} even when no derivatives are provided. Compared to \texttt{Data2LD} and \texttt{CollocInfer}, this approach numerically estimates a basis for the solution set of the linear ODE and then projects the observed data onto the approximate solution set using the \texttt{lm} command. On the other hand, \texttt{Data2LD} and \texttt{CollocInfer} add penalties to the least squares criterion to ensure sufficient adherence to the ODE \textit{post hoc}. This approach has the advantage of being much easier to implement, but it requires rigid adherence to the ODE, and cannot tackle non-linear problems like \texttt{CollocInfer}.

\section{Estimation for First Order Linear PDEs}

As discussed in Chapter 2, a similar framework can be used to perform estimation for PDEs in some cases. A complication is that for a PDE, the initial condition is a function rather than a constant. PDE problems cannot be tackled by either \texttt{Data2LD} or \texttt{CollocInfer}, while our approach exploits special structure to fit first order linear PDEs of the form:

\[
   \frac{\partial u}{\partial t} + c(x) \frac{\partial u}{\partial x} =0
\]



\subsection{The Transport Equation}
In Section \ref{sec:pde_fitting_strategy} the Transport Equation was introduced and a fitting strategy was sketched out. Recall that the Transport Equation is defined by:

\begin{equation}
\begin{cases}
       \frac{\partial u(x,t)}{\partial t} + \beta x \frac{\partial u(x,t)}{\partial t} &= 0 \\
       u(x,0) &= f(x)
\end{cases}
\end{equation}

\noindent In Chapter 2, a middle objective function $H(\beta)$ to estimate the parameter $\beta$ was defined, but no effort was made to fit the model. The objective function $H(\beta)$ was defined as the sum of squares:

\[
    H(\beta) = \sum [y_i - \hat{f}(x_i - \beta t_i)]^2.
\]

\noindent And so:

\[
   \frac{\partial H}{\partial \beta} = -\sum 2t_i\hat{f'}(x_i - \beta t_i)[y_i - \hat{f}(x_i - \beta t_i)]
\]

\noindent To compute the gradient of $H(\beta),$ the estimates of the functions $f(x)$ and $f'(x)$ associated with a given choice of $\beta$ are needed. This understates the difficulty however. The command \verb|smooth.spline| will only return the GCV score, not the sum of squared errors. Therefore we are forced to use a more complicated objective function than least squares unless a routine to compute them is available. Fortunately, Brent's Method can be used to minimise $H(\beta)$ instead. As can be seen in Figure \ref{fig:transport_profile} the objective function is irregular, and care must be taken that one is close to the optimal value already.  The estimated value of $\beta$ is very close to the original value of $\beta = 1.5.$

\begin{figure}[h]
\centering
\includegraphics[height=9cm]{nonlinear_regression/transport_equation_profile.pdf}
\caption{Plot of middle optimisation for the transport equation. Blue line denotes original parameter, red line denotes fitted estimate} \label{fig:transport_profile}
\end{figure}

\noindent Estimating $f(x)$ is harder than estimating $\beta$ as can be seen in Figure \ref{fig:transport_equation_fit}. While 200 samples are sufficient to estimate $\beta$ to a high degree of precision as shown in \ref{fig:transport_profile}, 2000 samples are needed to estimate $f(x)$ to a similar degree of accuracy.

\begin{figure}[h]
\centering
\subfloat[]{
   \includegraphics[height = 9cm]{nonlinear_regression/transport_200_samples.pdf}
}

\subfloat[]{
   \includegraphics[height=9cm]{nonlinear_regression/transport_2000_samples.pdf}
}
\caption{The estimates of $f(x)$ computed for various sample sizes. The plot (a) gives the result with 200 sample points, the plot (b) gives the result for 2000 sample points. The variable $z = x - \beta t, z$ was used as  the independent variable to avoid confusion between $x$ and $x - \beta t.$ Furthermore, the points used to fit $f(x)$ aren't displayed to reduce clutter and make it easier to compare the fitted curves with the original.} \label{fig:transport_equation_fit}

\end{figure}
\clearpage
\subsection{The Transport Equation with Space-varying Velocity}

The methodology will now be applied to a more complex PDE than the Transport Equation. Instead of having a constant velocity, the velocity will be allowed to vary with position by having the velocity equal to $\beta x$ instead of $\beta.$ This produces the following modified Transport Equation:

\begin{equation}
\begin{cases}
       \frac{\partial u(x,t)}{\partial t} + \beta x \frac{\partial u(x,t)}{\partial t} &= 0 \\
       u(x,0) &= f(x).
\end{cases}
\end{equation}

\noindent The problem of estimating $\beta$ for this PDE is ill-conditioned in the sense that \texttt{smooth.spline} will crash for some meshes. A more sophisticated non-parameteric fitting methodlogy is  needed if our approach is to  be consistently applied to first-order linear PDEs besides the Transport Equation.


\begin{figure}
\centering
\includegraphics[height=9cm]{transport_mod_profile.pdf}
\caption{Plot of outer optimisation for the modified transport equation with only 30 sample points. Blue line denotes original parameter, red line denotes fitted estimate} \label{fig:mod_transport_profile}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=9cm]{mod_transport_plot.pdf}
\caption{Plotted estimate of $f(x)$ for the modified transport equation with 30 sample points.}
\end{figure}

