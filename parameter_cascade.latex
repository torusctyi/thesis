\chapter{Hierarchial Estimation and the  Parameter Cascade}

This chapter opens with a discussion on fitting parameters to Partial Differential Equations. It is shown that a hierarchial method is needed, where the \emph{inner problem} consists of fitting the PDE to the data given the paremters, and the \emph{middle problem} in turn entails finding the best choice of parameters. The Parameter Cascade method for fitting penalised smoothing problems with unknown parameters is then introduced. 

A familiarity with numerical optimisation methods such as Gradient Descent, the Newton-Raphson Mathod, and Line Searches is assumed. Details are provided in Appendix \ref{chap:optimisation_methods}.

\section{Hierarchial fitting of a Partial Differential Equation}

A linear PDE that is analogous to the linear ODE used to model the Reflux data in Section \ref{sec:reflux_parametric_vs_data2ld} is the Transport Equation:

\[
    \frac{\partial u(x,t)}{\partial t} + \beta \frac{\partial u(x,t)}{\partial x} = 0
\]


A general solution to the Transport Equation is given by:

\[
   u(x,t) = f(x - \beta t)
\]

The function $f(\cdot)$ is unspecified. The solution $u(x,t)$ is constant along the rays $x = \beta t + C.$ The solution is an animation of the shape $f(x)$ moving to the right at fixed speed $\beta.$

The ODE $y'(t) + \beta y(t) = 0$ can be thought of as a simplifcation of the Transport Equation, where it is assumed that $u(x,t)$ only varies with time, and not with space. It is apparent that this PDE has a much richer solution structure than is the case for the ODE, which only has solutions of the form $Ae^{-\beta t}.$  Statistically speaking, fitting the Transport Equation to observed data is a semi-parametric problem because one of the parameters to be estimated is a function. The problem of fitting the Transport Equation is also a transformation model such as that used for the Box-Cox transformation, since the plot of $u(x,t)$ with respect to $x$ at a fixed time $t$ is a transformed version of $f(x),$ the curve at $t = 0.$

If the parameter governing the transformation process - $\beta$ - is known, $f(\cdot)$ is  reasonably easy to estimate. Suppose there were $n$ observed values $y_i$ at time $t_i$ and location $x_i.$  It has already been established that the value observed at a point $x$ at time $t$ depends only on $x - \beta t.$ The function $f(\cdot)$ could thus be estimated by non-parametrically regressing the observed values at $y_i$ against $x_i - \beta t_i$

What if $\beta$ were unknown? The above discussion suggests a hierarchial approach to estimation: for a given choice of $\beta,$ to fit an associated function $f(\cdot|\beta)$ using an appropriate non-parametric estimation method, and compute the associated least squares error. Let $H(\beta)$ be the  function that associates each $\beta$ with its sum of squared error:\footnote{In case the left hand side might be slightly unclear - for the $i$th observation, the associated function $f(\cdot|\beta)$ is evaluated at $x_i - \beta t_i.$}

\[
   H(\beta) = \sum_{i = 1}^n [y_i - f(x_i - \beta t_i |\beta)]^2
\]



The problem of minimising $H(\beta)$ is a non-linear least squares problem that is also a two level hierachial estimation problem. The  inner level consists of non-parametrically fitting a function to the set of points $\{(y_i, x_i -\beta_i)\}$ given $\beta.$ The associated sum of squared errors is then returned as $H(\beta)$. The outer level entails optimising the profiled objective function $H(\beta).$

This is a broad fitting strategy where different statistical and optimisation approaches can be swapped in and out as needed. There are several ways to tackle the inner function - LOESS; Kernel Regression; Penalised Splines, etc. The least squares loss function could be replaced with another one as suits the problem. There are many methods for optimising $H(\beta)$ that might be attempted - subgradient methods  if $H(\beta)$ is convex, Gradient Descent, Gauss-Newton Method, derivative-free methods and so on.


\section{The Two-Stage Parameter Cascade}


Consider the following  penalised regression problem:

\[
   PENSSE(f, \theta) = \sum_{i = 1}^N (y_i - f(t_i))^2 + \lambda \int_0^1 |T_\theta f|^2 dt.
\]

Here $T_\theta$ is some differential operator, that is parameterised by an unknown $\theta$ that is to be estimated. 

$T_\theta$ can be  an ordinary differntial operator or a partial differential operator; linear, quasi-linear, or nonlinear.

There are two statistical objects to be estimated here: the parameter $\theta,$ and the function $f(t).$ 

Ramsay and Cao propose the following hierarchial approach to estimation\cite{cao2007parameter}: Given a fixed value of $\theta,$ let $f(t|\theta)$ denote the function that minimises $PENSSE(f, \theta)$ For a given value of $\theta,$ it's associated mean square error is then  defined by:

\[
   MSE(\theta) = \frac{1}{N}\sum_{i=1}^N[y_i - f(t_i|\theta)]^2
\]

By making $f(t)$ dependent on $\theta$, the fitting problem has been reduced to a  non-linear least squares problem.

This leaves the issue of estimating the optimal value of $\theta$ - Ramsay and Cao propose the use of Gradient Descent.

For a given value of $\theta,$ $f(t|\theta)$ is found. These two values together are then used to compute $MSE(\theta)$ and $\nabla MSE(\theta).$ Finally, a new value of $\theta$ is computed by perturbing $\theta$ in the direction of the gradient.This scheme is sketched out in Figure ~\ref{fig:twoStageParamSimplified}.

It is assumed that $f(t)$ can be represented by a finite vector $\mathbf{c}$ associated with an appropriate basis. This leads to a pair of nested optimisation problems: the \emph{Inner Optimisation} involves finding the value of $\mathbf{c}$ that minimises the penalised least squares criterion given $\theta,$ and the \emph{Middle Optimisation} entails finding the  of value of $\theta$ that minimises $MSE(\theta).$ 

There is thus a `cascade' of estimation problems, where the results of the lower level estimation problems feed back in to the higher level ones.

Note that every time a new value of $\theta$ is introduced, the associated function $f(t|\theta)$ must be computed from scratch. The middle optimisation can thus generate many inner optimisation subproblems as the parameter space is explored, and these in turn could require multiple iterations to complete if no explicit formula for $\mathbf{c}$ given $\theta$ is available. 

Figure \ref{fig:twoStageParamSimplified} is a highly idealised sketch of the Parameter Cascade. The main abstraction is that the step of computing $f(t|\theta)$ is presented as a single atomic and organic step, even though it could be a complex process in its own right. This risks masking some of the comptuational work that is happening. A more realistic  description is provided  in Figure ~\ref{fig:twoStageParam}. In this thesis, Parameter Cascade problems that cannot be differentiated easily or at all are considered.

\begin{figure}
   \centering
   \includegraphics[width = 13cm]{/home/padraig/programming/R/latex/chapter_2/2_stage_parameter.pdf}
   \caption{Two Stage Parameter Cascade (Simplified)}
      \label{fig:twoStageParamSimplified}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[width = 13cm]{/home/padraig/programming/R/latex/chapter_2/2_stage_parameter_full.pdf}
   \caption{Schematic of the Two Stage Parameter Cascade With the Inner Optimisation Visible}
\label{fig:twoStageParam}
\end{figure}


\section{Three Stage Parameter Cascade}

Up to this point, the structural parameter $\lambda$ has been treated as fixed. But it is possible to extend the Parameter Cascade to estimate $\lambda.$

It is necessary to introduce  an \emph{Outer Criterion} $F(\lambda)$ that determines how good a given choice of $\lambda$ is.

A common choice of outer criterion is  Generalised Cross Validation\cite{cao2007parameter, ramsay2007parameter}. 

Just as the problem of fitting a function $f(\cdot|\theta)$ can generate an optimisation subproblem, that of fitting a third level in the cascade can generate a series of subproblems to find the best parameter choice associated with a given value of $\lambda,$ which in turn generates a series of subproblems to find the fitted function as the parameter space is explored.

As stated in Chapter 1, neither the \texttt{FDA} nor \texttt{Data2LD} packages  implement the three stage parameter cascade. They instead expect practioners to find the best choice of $\lambda$ by cycling through a set of predetermined values or even just employing manual adjustment. There is no guarantee that the optimal choice of $\lambda$ would lie in a finite set of predetemined values. Manual adjustment is slow and cumbersome compared to an automated optimisation routine.

\section{Investigating the \texttt{Data2LD} Package} \label{sec:data2ld_investigation}

Data2LD uses a sophisticated two-level parameter cascade algorithm to fit parameters to the data, which is briefly described here.

The inner level of the parameter cascade is implemented by the eponymous \texttt{Data2LD} routine. The middle level is implemented by the \texttt{Data2LD.opt} command.\cite{data2ld} The outer level of optimisation for choosing the trade-off parameter $\lambda$ is not implemented.\footnote{For \texttt{Data2LD}, the smoothing parameter is written in terms of $\rho = \lambda/(1 + \lambda).$ The term $\lambda$ is retained for the sake of simplifying the exposition.}

The \texttt{Data2LD} function is written in what might be called `R Style' -  upon calling the method, it returns a list with a of number computed quantites and statistics. The associated Mean Square Error (MSE),\footnote{To be clear here, the MSE returned is the MSE associated with the choice of paremeters.  The command \texttt{Data2LD} fits a function using the  choice of parameters passed to it, and then reports the associated MSE alongside other related values.} the gradient of MSE and the Hessian of MSE will always be computed and returned whether one needs them or not.

Only the \texttt{Data2LD.opt} command is  investigated in detail here. The middle level of the Parameter Cascade is generally the easier to implement than the inner level. When implementing the middle level,  one can generally treat the lower level as a `black box' that accepts a given choice of parameters as inputs, and then returns the value of the objective function and sometimes derivatives as outputs. There is plenty of existing methods for tackling such optimisation problems.

\subsection{How \texttt{Data2LD} Estimates Parameters}

The code for Data2LD can be difficult to understand unfortunately. While software with such powerful features was probably inevitably going to be complex, the authors compound the issue by not heeding best practices recommended for making code easy to read and maintain. For example, Data2LD hardcodes unnamed constants into the code. Allowing such 'Magic Numbers' is strongly discouraged because it makes code more error prone and difficult to understand.\cite{mcconnell2004code}

The search directions used by the  \texttt{Data2LD.opt} command are the gradient descent direction:

\begin{equation} \label{eqn:grad_descent}
    \mathbf{p}_n = -\mathbf{g}_n \tag{\textbf{S1}}
\end{equation}

and the Newton Direction:
\begin{equation} \label{eqn:newton_method}
    \mathbf{p}_n = - \mathbf{H}_n^{-1}\mathbf{g}_n \tag{\textbf{S2}}
\end{equation}

Data2LD uses four tests to determine how good a step is:\footnote{\texttt{Data2LD} actually tests for the negation of \ref{eqn:t3} and \ref{eqn:t4}. For the sake of consistency the logical negations of the two tests used by \texttt{Data2LD} are presented  here so that passing a test is consistently a good thing and failing consistently represents unsatisfactory or pathalogical behaviour.}

\begin{align*}
\end{align*}
\begin{itemize}

\item First Wolfe Condition (checks for sufficient decrease in the objective function): 

\begin{equation} \label{eqn:t1}
   f(\mathbf{\theta}_n + \alpha_n\mathbf{p}_n) \leq f(\mathbf{\theta}_n) + c_1\alpha_n \mathbf{p}_n^\top\mathbf{g}_n \tag{\textbf{T1}}
\end{equation}

\item Second  Wolfe Condition (checks for sufficient decrease in curvature): 

\begin{equation} \label{eqn:t2}
   |\mathbf{p}_n^\top\nabla  f(\mathbf{\theta}_n+ \alpha_n\mathbf{p}_n)| \leq c_2 |\mathbf{p}_n^\top\nabla  f(\mathbf{\theta}_n)| \tag{\textbf{T2}}
\end{equation}

\item Has the function even decreased compared to the previous iteration?

\begin{equation}\label{eqn:t3}
   f(\mathbf{\theta}_n + \alpha_n\mathbf{p}_n) \leq  f(\mathbf{\theta}_n) \tag{\textbf{T3}}
\end{equation}
   
\item Has the slope along the search direction remained nonnegative? 

\begin{equation}\label{eqn:t4}
    \mathbf{p}_n^\top\nabla  f(\mathbf{\theta}_n+ \alpha_n\mathbf{p}_n)  \leq 0 \tag{\textbf{T4}}
\end{equation}

\end{itemize}

These tests are illustrated in Figure \ref{fig:data2ld_line_search_tests}. Written in terms of $\phi(\alpha) = f(\mathbf{\theta}+ \alpha \mathbf{p}_n)$ the tests are:

\begin{align*}
   \phi(\alpha_n)     &\leq \phi(0) + c_1\alpha_n \phi'(0) \tag{\textbf{T1\textquotesingle}}\\
   |\phi'(\alpha_n)|  &\leq c_2|\phi'(0)|                  \tag{\textbf{T2\textquotesingle}} \\
   \phi(\alpha)       &\leq \phi(0)                        \tag{\textbf{T3\textquotesingle}} \\
   \phi'(\alpha)      &\leq 0                              \tag{\textbf{T4\textquotesingle}} \\
\end{align*}
     

If \ref{eqn:t1} and \ref{eqn:t2} are satisfied, then the line search has converged completely. If \ref{eqn:t3} has failed, this represents a total failure because it means the line search has failed to actually produce any improvement in the objective function. A failure in \ref{eqn:t4} means the function has overshot a critical point.\footnote{If \ref{eqn:t4} fails, this implies that $\mathbf{p}_n^\top\nabla  f(\mathbf{\theta}_n+ \alpha_n\mathbf{p}_n)$ and $\mathbf{p}_n^\top\nabla  f(\mathbf{\theta}_n)$ are of opposite sign since $\mathbf{p}_n$ is choosen so that $\mathbf{p}_n^\top\mathbf{g}_n <0.$ The Intermediate Value Theorem means there is an $\bar{\alpha}$ between $0$ and $\alpha_n$ such that $\mathbf{p}_n^\top\nabla f(\mathbf{\theta}_n+ \bar{\alpha}\mathbf{p}_n) = 0,$ so that there is a critical point on the line segment between $\mathbf{\theta}_n$ and $\mathbf{\theta}_n+ \alpha_n\mathbf{p}_n.$}

The use of four tests is a little unusual here. The literature suggests that only the Wolfe Conditions \ref{eqn:t1} and \ref{eqn:t2} are needed as discussed in Section \ref{sec:line_search_methods}.  \texttt{Data2LD.opt} is designed to be  robust against the possibility that the objective function mightn't behave as predicted by the computed gradient and Hessian.


Depending on the outcome of the tests, Data2LD chooses the stepsize as follows:

\begin{itemize}

\item If \ref{eqn:t1}, \ref{eqn:t2}, and \ref{eqn:t3} are passed, the algorithm terminates.

\item If \ref{eqn:t1} and \ref{eqn:t1} are passed, or  \ref{eqn:t4} is passed; but  \ref{eqn:t3} is failed, it means that the slope is satisfactory, but the function has increased rather than decreased. Data2LD reduces the step size.

\item If all four tests are failed, then the newest point is unsuitable entirely. Data2LD falls back on interpolation to try to find a critical point of $\phi(\alpha),$ falling back on  quadratic interpolation methods if necessary.\footnote{The line search code for the \texttt{Data2LD} is lightly commented and dense, all that one can be strictly certain of is that the method uses radicals to compute the next value of $\alpha,$ falling back on solving a linear equation if necessary. Getting the root of a quadratic is equivalent to finding a critical point of a cubic, and solving a linear equation is equivalent to finding the critical points of a quadratic.}

\end{itemize}

\begin{figure}
\centering
\includegraphics[height = 10cm]{data2ld_tests.pdf}
\caption{Point A is the initial point. Point B passes  \ref{eqn:t1} with $c_1 = 0.5$ and passes \ref{eqn:t2} with  $c_2 = 0.9.$  Point C fails \ref{eqn:t1} with $c_1 = 0.5$  and also fails \ref{eqn:t3}, but  passes \ref{eqn:t4}, and passes \ref{eqn:t2} with  $c_2 = 0.9.$ Point D fails all four tests.}
\label{fig:data2ld_line_search_tests}
\end{figure}

If the line search succeeds in reducing the objective function, Data2LD uses the Newton search direction for the next iteration. If the line search makes the objective function worse, the Gradient Descent direction is used. In the event of the line search making the objective function worse twice in a row, Data2LD returns an error.

Somewhat peculiarly, \texttt{Data2LD} does not make use of $\phi''(\alpha)$ despite being able to compute it easily.\footnote{Differentiating the expression $\phi'(\alpha) =\mathbf{p}_n^\top\nabla f(\mathbf{x}_n + \alpha \mathbf{p}_n)$ with respect to $\alpha$ yields that $\phi''(\alpha) = \mathbf{p}_n^\top \mathbf{H}(\alpha) \mathbf{p}_n,$ where $\mathbf{H}(\alpha)$ denotes the Hessian of $f$ evaluated at $\mathbf{x}_n + \alpha \mathbf{p}_n.$}  One would think that the Newton-Raphson Method would  be the first approach attempted to perform the line search before resorting to interpolation-based methods since it's both simpler to implement and faster to converge. The effort of computing $\phi''(\alpha)$ is mostly a sunk cost because of how the interface of \texttt{Data2LD} is defined.





