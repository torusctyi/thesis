\chapter{A Two Level $L_1$ Parameter Cascade Using the MM Algorithm.} \label{chap:mm_methods}

In the previous chapter, Brent's method was introduced and was used to tackle Parameter Cascade problems where the middle level uses a loss function which is difficult or to differentiate or simply has no well-defined derivative everywhere. It was remarked  that Brent's method ensures that the different elements of the Parameter Cascade tend to be loosely coupled from each other and this allows one to combine different fitting methodologies for different levels with straightforwardly.

Here these ideas are developed further. First, the $L_2$ based penalised fitting method is extended to the $L_1$ case. This new method is then used alongside Brent's method to implement a two level Parameter Cascade with $L_1$ loss functions at both levels. 

\section{$L_1$ Estimation for the Inner Problem} \label{sec:mm_l1_problem}

Brent's method is designed to optimise  real-valued functions over a real interval. In the previous it was extended to functions that take more than one real argument by optimising over each coordinate separately when the Cauchy likelihood was optimised. However, there is no guarantee that this approach will perform well, and it can even fail entirely for functions that have an exotic topography or multiple local optima arranged unusually\footnote{ Consider for example the problem of finding the minimum of  the function $f(x) = x \sin(x)$ over the interval $[0,13].$ It is easy to see that the minimum is not on the boundary points of the interval because $f(0) = 0, f(6) = -1.67,$ and $f(13) = 5.45.$ Brent's method fails to find the minimum. It claims the optimal value is given by $f(4.9) = -4.81$ though $f(11) = -10.99.$}. Even in the best case, optimising over each coordinate generates its own optimisation sub-problem, which has the cumulative effect of increasing the running time of the algorithm. Brent's method further requires the specification of a bounding box that contains the optimal  point since it uses bisection, and that is harder and harder to do as the number of dimensions increases. All of these considerations mean that Brent's Method is highly unsuitable for performing $L_1$ fitting over a space of functions which tend to have a large number of dimensions - by definition, there is one  dimension introduced for each basis function used. Likewise, the non-differentiabilty of the absolute value function means that other approaches that implicitly rely on differentiabilty such as parabolic interpolation are inadvisable.

Instead a  different approach will be employed, a generalisation of the Iteratively Re-weighted Least Squares algorithm for computing the $L_1$ median of a set of $N$ items $\{x_1, \dots, x_N\}$ to which an $L_1$ norm can be associated. The $L_1$ median is defined as  the object $x$ that minimises $\sum_{i=1}^N |x - x_i|.$   We will start by describing  how IWLS can be used to compute the $L_1$ median of a set of real numbers. We will further show that this as an example of what is known as an MM algorithm, and then proceed to straightforwardly extend this MM algorithm to produce a modified Penalised Sum of Squares problem that can be iteratively solved and re-weighted to find the function that minimises a penalised $L_1$ norm. 

\subsection{An MM Algorithm For Computing the Median}
Suppose that given a set of numbers $\{x_1, \dots, x_N\}$ , one wished to find the number $x$ that minimised the $L_1$ distance between them:

\[
   SAE(x) = \sum_{i=1}^N |x_i - x|
\]

It is well known that $SAE(x)$ is minimised by the sample median of the numbers \cite{small1990survey}\footnote{This is asserted without proof in \cite{small1990survey}, probably because the proof tends to be simultaneously awkward but trivial to those familiar with it. The amount of work required to demonstrate that the sample median minimises $SAE(x)$ is greatly reduced if one notes that $SAE(x)$ is a convex function in $x.$ Any local minimum of a convex function is also a  global minimum\cite{boyd2004convex}, so one only needs to show that for any sufficiently small $\epsilon$ that $SAE(\bar{x}) \leq SAE(\bar{x} \pm \epsilon),$ where $\bar{x}$ is the sample median.}. The usual approach to computing the sample median - sorting the numbers and taking the one in the middle -  cannot be generalised to FDA problems, so we will use a different approach. The main difficulty is that the function $SAE(x)$ is not everywhere differentiable, which means that the usual derivative-based techniques such as gradient descent or Newton's method can't work. Instead an approach known as Majorise-Minimise or the MM Algorithm  will be used\cite{hunter2004tutorial,lange2004optimization,lange2010numerical}.  For a given iterate $x_n$, a function $M(x|x_n)$ is required with the following properties:

\begin{align*}
     M(x|x_n)    &\geq SAE(x) \\
     M(x_n| x_n) &= SAE(x_n)
\end{align*}

The function $M(x|x_n)$ is said to majorise $SAE(x).$ The next iterate $x_{n+1}$ is then found as the value of $x$ that minimises $M(x|x_n).$ Thus:

\begin{align*}
   SAE(x_{n+1}) &\leq M(x_{n+1}|x_n) \\
                &\leq M(x_n| x_{n+1}) \\
                &= SAE(x_n) \\
\end{align*}

If such a function $M(x|y)$ could be determined such that $M(x|y)$ would be straightforward to minimise,  it is then possible to easily produce a sequence of iterates $x_n$ such that $SAE(x_{n+1}) \leq SAE(x_n)$ for all $n.$ This pattern of monotone improvement in the objective function is similar to the EM Algorithm. In fact, the EM algorithm is a special case of the MM algorithm\cite{wu2010mm} \footnote{When applied to maximisation problems, MM instead stands for Minorise-Maximise. This case is the same except the surrogate function is required to be less than or equal to the objective function and it is maximised on each iteration. Thus, each iteration drives the objective function upwards.}.

The most important question associated with the MM algorithm is the construction of the majorising function because this tends to take up the bulk of the effort. Once the majoriser has been found, the algorithm is generally straightforward to implement, as  will be seen shortly \cite{lange2007slides, hunter2004tutorial}.  Verifying a potential majoriser is usually straightforward, finding one in the first place is more difficult.The EM algorithm for example takes advantage of the probabilistic structure of the problem and Jensen's inequality \footnote{The EM algorithm is intended to maximise the log-likelihood and drives it upwards on each iteration, so it's an example of a Minorise-Maximise algorithm.}. For an $L_1$ problem, the usual approach  is to employ the Arithmetic Mean-Geometric Mean inequality \cite{lange2007slides}. Only  the AM-GM inequality in its simplest form is required here, that the geometric mean of two numbers is less than or equal to  their arithmetic mean:

\[
    \sqrt{xy} \leq \frac{x + y}{2}
\]

It's worth noting that the AM-GM inequality is in fact a special case of Jensen's Inequality since the log function is concave:

\begin{align*}
    \log(\frac{x + y}{2}) &\geq \frac{\log{x}}{2} + \frac{\log{y}}{2} \\
                                   &= \log{\sqrt{x}} + \log{\sqrt{y}}\\
                                  &= \log{\sqrt{xy}}
\end{align*}

It is possible  to exploit the AM-GM inequality to majorise an $L_1$ regression problem by a weighted $L_2$  problem. One can represent the $L_1$ norm as a geometric mean, which then allows for the $L_1$ norm to majorised and seperated by  a weighted sum of squares. Given an interate $x_n,$ the AM-GM inequality implies that:

\begin{align*}
    |y - x| &= \sqrt{(y - x)^2} \\
            &= \sqrt{\frac{(y-x)^2}{|y - x_n|}|y-x_n|} \\
            &\leq \frac{1}{2}\left(\frac{(y-x)^2}{|y - x_n|} + |y-x_n|\right)
\end{align*}

This in turn implies that:

\begin{align*}    
     \sum |x_i - x| &\leq  \frac{1}{2}\sum\left(\frac{(x_i-x)^2}{|x_i - x_n|} + |x_i-x_n|\right) \\
                   &=        \frac{1}{2}\sum\frac{(x_i-x)^2}{|x_i - x_n|} +  \frac{1}{2}\sum\left(|x_i-x_n|\right)
\end{align*}

The $L_1$ problem is thus  majorised by a weighted least squares problem. The $\frac{1}{2}\sum|x_i-x_n|$ term is constant with respect to $x,$ so neglecting it makes no difference to the choice of $x$ that is optimal. Likewise, multiplying the weighted least squares problem by a positive constant doesn't change the optimal value either, so the $\frac{1}{2}$ term can be eliminated by multiplying by $2$.  The optimal value  $x_{n+1}$ can thus be found by minimising this weighted least squares score:

\[ 
   \sum \frac{(x_i-x)^2}{|x_i - x_n|}
\]

The algorithm thus consists of finding the value of $x$ that minimises the least squares error inversely weighted by the residuals from the previous iteration. 

\subsection{Penalised $L_1$ Fitting} \label{sec:mm_l1_algorithm}
For the case of penalised regression, the penalised sum of absolute errors is defined by:

\[
  PENSAE(f|\theta, \lambda) = \sum |x_i - f(t_i)| + \lambda \int |Tf|^2 dt
\]

Here $T$ is used instead of $L$ to  denote a differential operator that might not necessarily be linear.\footnote{In some situations $T$ could even be an integral operator. This could easily be the case for example if the observed values were the measured velocities of a vehicle, and the penalty was intended to impose constraints on quantities such as the distance travelled or fuel consumed} As before, this can be majorised by a weighted sum of a residual-weighted penalised sum of squared errors, and a vestigial $\sum |x_i - f_n(t_i)|$  term that is only included for completeness and  can be safely ignored in the course of the actual optimisation.

\begin{align} \label{eqn:wpensse}
    PENSAE(f) &\leq \frac{1}{2}WPENSSE(f|f_n, \theta, 2\lambda) + \frac{1}{2} \left(\sum |x_i - f_n(t_i)| \right)\\
              &= \frac{1}{2}\left( \sum \frac{[x_i - f(t_i)]^2}{|x_i - f_n(t_i)|} + 2 \lambda \int |Tf|^2 dt\right) +  \frac{1}{2} \left(\sum |x_i - f_n(t_i)|\right)
\end{align}

To find the function that minimises the penalised $L_1$ error, one repeatedly finds the function that minimises $WPENSSE$ with the previous set of residuals used as inverse weights. This produces a sequence of fitted functions for which the penalised sum of absolute errors is monotonically forced downwards. 

\subsection{Discussion} \label{sec:discuss}

The  sequence of penalised errors $PENSAE(f_n)$ is monotone decreasing but cannot be less than zero, so it is a bounded monotone sequence. The Monotone Convergence Theorem  for sequences of real numbers\cite{rudin1964principles} thus guarantees that a given generated sequence $PENSAE(f_n)$  will always converge to  a limit. There are two caveats. First, the sequence might converge to a different point depending on the starting values - there is no guarantee that the sequence will converge to the  lowest possible value of $PENSAE.$ Second, there is no guarantee that the underlying sequence of functions will converge, and may just oscillate between several points. The sequence $-1, 1, -1, \dots$ does not converge but the associated sequence of absolute values $1,1,1, \dots$ does.


This approach of associating the objective function with more standard problem that acts as a surrogate is employed in the literature on the EM Algorithm. For example, in the introductory chapter of \cite{mclachlan2007algorithm}, the authors discuss how a multinomial estimation problem can be transformed into a binomial problem with missing data by artificially splitting one of the cells; they then construct a simple iterative EM scheme that can then be repeatedly iterated to estimate parameters for the original multinomial. They even remark that the the surrogate problems associated with EM algorithms tend to be easy to solve using existing tools in the field. Likewise,  the $L_1$ problem has been replaced  here with a  surrogate sequence of weighted $L_2$ problems that can easily solved using the \texttt{FDA} package. Since the \texttt{FDA} package does much of the heavy lifting, the  actual code for implementing penalised $L_1$ regression is brief.

The literature on the  MM algorithm remarks that it is simple to implement and  good at tackling high dimensional penalised regression, though convergence can be slow \cite{wu2010mm}. These claims are borne out when the convergence of the method is examined in Section \ref{sec:testing} below.

\subsection{Testing the Algorithm on the Melanoma Data} \label{sec:testing}

Since minimising $PENSAE$ is a  problem over many dimes ions, plotting the objective function to  verify that the optimal function has been found isn't possible. Instead the MM algorithm described in Section \ref{sec:mm_l1_algorithm} will be tested by applying it to the  melanoma data perturbed by random noise. Further, the convergence of the algorithm for the original melanoma dataset will be examined. 

Figure \ref{fig:mela_l1_l2} presents the $L_1$ and $L_2$ inner fits to the melanoma data corrupted by Cauchy distributed noise. The value of $\omega$ was held fixed at  value of $0.3,$ which was chosen as being roughly the average of the two different estimates of $\omega$ presented in Figures \ref{fig:mela_omega} and \ref{fig:mela_mad_omega} from the previous chapter. It is apparent from the Figure \ref{fig:mela_l1_l2} that the MM fit is robust against outliers, tends to ignore more deviant points, and even manages to remain similar to the original fit. The  least-squares fit tends to chase the heavy-tailed noise on the other hand. This is strong evidence that the curve that minimises $PENSAE$ has been found and that the method has been implemented correctly.



Figures \ref{fig:sae_plot} and \ref{fig:pensae_plot} plot the convergence of $SAE$ and $PENSAE$ over the course of the algorithm.  Note that the PENSAE statistic doesn't quite actually converge monotonically as the theoretical analysis predicted. Instead, it fluctuates before settling down to the typical and expected pattern of monotone decline. Upon investigation, it was determined that over the first handful of iterations the range of the weights applied to the observations  on each iteration,  that were computed using the residuals from the previous iteration, grew very rapidly. By the fourth iteration, the lowest weight is equal to $1.48,$ and the highest was equal to $4.8 \times 10^6.$ It seems that this rapid and large change produces qualitative changes in behaviour before the algorithm manages to  `burn in'. It is likely that observations with low weights are being effectively censored after a few iterations due to roundoff error. It was found that imposing a minimum threshold for the weights by adding a constant to all the residuals before proceeding to computing the weights smooths out this behaviour, but doesn't eliminate it entirely.


Figure \ref{fig:coef_plot} plots the convergence of the coefficient vectors $\mathbf{c}_n.$ This log-plot suggests  that  the sequence of fitted coefficient vectors $\mathbf{c}_n$ converges linearly since $\|\mathbf{c}_{n+1} - \mathbf{c}_n\| \approx C\|\mathbf{c}_{n} - \mathbf{c}_{n-1}\|$ as $n \rightarrow \infty.$

\begin{figure}
    \centering
    \includegraphics[height=10cm]{mela_l1_l2_plot.pdf}
    \caption{Comparison of $L_1$ and $L_2$ inner fits to Cauchy perturbed data with $\omega$ fixed at 0.3}
\label{fig:mela_l1_l2}
\end{figure}


\begin{figure}
\centering
   \includegraphics[height=9cm]{/home/padraig/latex/mela_l1_convergence_plot.pdf}
   \includegraphics[height=9cm]{/home/padraig/latex/mela_l1_convergence_log_plot.pdf}
   \caption{Plot of values and log differences for SAE Statistic}
   \label{fig:sae_plot}
\end{figure}

\begin{figure}
\centering
   \includegraphics[height=9cm]{mela_coef_convergence_plot.pdf}
   \caption{Plot of log norm differences for coefficients. Note that they tend to settle on a line.}
    \label{fig:coef_plot}
\end{figure}

\begin{figure}
\centering
   \includegraphics[height=9cm]{mela_pensae_plot.pdf}
   \caption{Plot of PENSAE statistic as the algorithm proceeds.}
   \label{fig:pensae_plot}
\end{figure}

\clearpage



\section{The Two Level Parameter Cascade with $L_1$ Norm}

The inner problem  of the parameter cascade is  a semi-parametric least squares regression model. The fitted function is modelled as a weighted sum of a solution to the differential equation (parametric), and a non-parametric residual. The $\lambda$ term governs how big the residual is allowed to be relative to the the least squares error term

If the usual least-squares error function is used, the inner problem will probably struggle with outliers and heavy tailed errors as is the case for any form of least-squares regression. 

For high order differential operators like that used to model the melanoma data, there are  many degrees of freedom associated with the differential operator's solution set. The $\omega$ and $\lambda$ parameters don't strongly constrain the lower level of the cascade.  There is thus little capacity for the higher levels of the cascade to restrain the lowest level through altering the lambda and omega parameters and the parameter cascade must use robust estimation at every level.

In Chapter \ref{chap:brent}, it was  discussed how Brent's Method can be used to tackle the middle problem without derivatives and then used this approach to optimise a highly irregular loss function. In Section \ref{sec:mm_l1_problem},  the MM algorithm was used to optimise the inner problem with an $L_1$ norm. 

Combining the two methods, it is very straightforward to implement a two-level parameter cascade with $L_1$ errors at both levels. 

In Figure \ref{fig:mela_l1_cascade}, the result of fitting a two level $L_1$ Parameter Cascade with $L_1$ errors is plotted. It can be seen that the $SAE(\omega)$ function is irregularly shaped. In Figure \ref{fig:mela_l1_l2_cascade} both the $L_1$ and $L_2$ fits to Cauchy-perturbed melanoma data are shown.  Figure \ref{fig:mela_l1_l2_combiations} plots the results of applying the $L_1$ and $L_2$ Parameter Cascades to the original and perturbed Melaonoma data, alongside mixed versions where the $L_1$ loss function is used for the inner fitting and $L_2$ loss function for the middle fitting and vice versa.



\begin{figure} 
\centering
    \includegraphics[height=9cm]{/home/padraig/latex/mela_omega_mae_profile.pdf}
    \includegraphics[height=9cm]{/home/padraig/latex/mela_omega_mae_fit.pdf}
    \caption{Fitting an $L_1$ Parameter Cascade to the Melanoma Data}
    \label{fig:mela_l1_cascade}
\end{figure}

\begin{figure} 
\centering
 \includegraphics[height=9cm]{/home/padraig/latex/mela_l1_l2_cascade_plot.pdf}
 \caption{$L_1$ and $L_2$ Parameter Cascades with the same perturbed data as in Figure \ref{fig:mela_l1_l2}. Compare the $L_1$ curve in this plot with the one in Figure \ref{fig:mela_l1_cascade}.}
 \label{fig:mela_l1_l2_cascade}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics[height=9cm]{mela_l1_l2_combinations.pdf} 
\includegraphics[height=9cm]{mela_l1_l2_combinations_perturbed.pdf}
\caption{All possible combinations of $L_1$ and $L_2$ loss functions that can be used for the Parameter Cascade. The top plot applies them to the orginal melanoma data, the bottom to the same perturbed data as in Figures \ref{fig:mela_l1_l2} and \ref{fig:mela_l1_l2_cascade}}
\label{fig:mela_l1_l2_combiations} 
\end{figure}

\newpage
\clearpage
\section{Accelerating The Rate of Convergence}

The MM Alogrithm is very sluggish, and this is a well known weakness of both itself and the EM algorithm. The literature however suggests that this problem could be easily ameliorated in this particular case because of a special feature present.\cite{mclachlan2007algorithm} In practice one doesn't  want to fit the full model, but only wants to compute an associated summary statistic that determines how good a given choice of parameters is. It will often be the case that  only the value of $PENSSAE$ or $GCV$ or $SAE$  associated with a given choice of parameters is required as inputs to an optimisation routine, and it is not desirable to iterate until the full model converges if this effort can be avoided. 

MacLanan and Krishnan\cite{mclachlan2007algorithm} discuss the situation where one only wants to compare the likelihoods between a restricted model and a full model. They suggest the use of sequence acceleration methods to rapidly extract the likelihoods \cite{mclachlan2007algorithm} instead of running the EM algorithm to completion since  the full models aren't needed. The literature on the MM algorithm claims that acceleration methods for the EM algorithm translate quite easily to the MM case \cite{wu2010mm}. On this basis, we explored whether this approach might be applied here.

The approach employed is known as Aitken Acceleration\cite{dempster1977maximum, mclachlan2007algorithm}. Suppose that there is a sequence $x_0, x_1, x_3, \dots$ converging to a limit $x^*$. Aitken's method makes the ansatz that $ x_{n+1} - x^* \approx C(x_n - x^*)$  for some constant $C$. Many iterative algorithms in statistics exhibit this pattern as discussed in Section \ref{sec:discuss}. 
This suggests the following equation:

\[
  \frac{x_{n+1} - x^*}{x_{n} - x^*} \approx \frac{x_{n} - x^*}{x_{n-1} - x^*}
\]

Solving for $x^*$ gives the accelerated sequence. 

There is an equivalent definition that is easier to generalise \cite{isaacson2012analysis}. Consider a sequence defined by functional iteration so that $x_{n+1} = F(x_n)$ for some function $F(\cdot).$ Define the error sequence by $e_{n} = x_{n+1}- x_n = F(x_n) - x_n.$ The function $g(x) = F(x) - x$ returns the error associated with any value, and the limit of the sequence satisfies $g(x^*) = 0.$ Suppose one knew the inverse of $g(x),$ which will be denoted by $h(e).$ Then $x^*$ could be found by evaluating $f(0).$ The next best thing would be to use the values of the sequence to approximate $h(e),$ and then evaluate this approximate function at zero instead. The Aitken method approximates $h(e)$ by linear interpolation between $(e_n, x_n)$ and $(e_{n-1}, x_{n-1}),$ and then evaluates this approximation at $e = 0.$ 

\newpage

\subsection{Illustrative Example: Using Imputation to Fit an ANOVA Model With Missing Data}

For illustrative purposes, we will make use of an example from chapter 2 of \cite{mclachlan2007algorithm}. The authors discuss fitting an ANOVA model to a factorial experiment where some of the values are missing. They proceed by using the fitted model to estimate the missing values; fitting the model again with the new imputed values; and using the new fitted values in turn to again update the estimates of missing values. The process is repeated until convergence. In the text, the authors do not work with likelihood or any probabilistic models and treat the question as purely a regression problem. This is similar to our $L_1$ fitting problem.

The authors' example was implemented again in R.\footnote{The $SSE$ statistic here is different from the $RSS$ statistic presented in the text this example was taken from. The new code converges to the same estimates as in the original, so the example has been re-implemented correctly. It was not possible to determine with what degrees of freedom  $RSS$ was associated with.}. For each iteration, the $SSE$ statistic was computed. This defines an associated sequence $\{SSE_1, SSE_2,\dots, SSE_n, \dots\}.$ Applying Aitken's method to this sequence produces a new sequence $\{ASSE_n\}.$ As can be seen in Figure \ref{fig:sse_error} and Table \ref{tab:sse_error}, the accelerated sequence converges far more quickly to the limit of the $\{SSE_i\}$ sequence than the original sequence.

\subsection{Generalisations}

Exploring more powerful methods than Aitken's method can be justified in two circumstances. The first is that if one is running the algorithm over and over again such that an increase in speed over many iterations means the effort invested is worth it. This might be the case for example if one  wanted to use the bootstrap to model the distribution of  a likelihood ratio statistic computed using the EM Algorithm as previously described. The second is if the sequence is difficult to accelerate. In the context of accelerating the MM fitting algorithm introduced in Section \ref{sec:mm_l1_problem}, it shall be seen that both conditions apply.




As a field of study, sequence acceleration is closely related to time series analysis. A generic first order autoregressive model is given by:

\[
   x_{n+1} = f(x_n, n) +  \epsilon_n
\]

Consider the case where there are  both no random errors so that $\epsilon_n$ is always zero, and the sequence converges to a limit. Here, the problem of determining the long term value of the sequence from a set of observations is equivalent to that of accelerating the sequence. If the specific form of $f(x_n, n)$ is known, there can often be a specific acceleration method that can exactly extract the limit.  For  illustration, suppose there were a sequence of the following form, but the parameters $\beta_0$ and $\beta_1$ were unknown:

\begin{equation} 
  x_n = \beta_0 + \frac{\beta_1}{n} \label{eqn:rec_seq}
\end{equation}

As $n$ goes to infinity, $x_n$ converges to $\beta_0.$ It is not difficult to show that the limit $\beta_0$ can be found by applying  the following sequence transformation:

\begin{equation}  
   \begin{cases} 
      \begin{aligned}
       \hat{\beta}_{1,n} &= \frac{x_n - x_{n+1}}{\left(\frac{1}{n} - \frac{1}{n+1}\right)} \\
       \tilde{x}_n &= x_n - \frac{\hat{\beta}_{1,n}}{n}
     \end{aligned}
  \end{cases} \label{eqn:rec_trans}
\end{equation}

If the transformation (\ref{eqn:rec_trans}) is applied to a sequence of values $x_1, x_2, \dots, x_n, \dots$ that is of form (\ref{eqn:rec_seq}), then the transformed sequence $\tilde{x}_1, \tilde{x}_2, \dots, \tilde{x}_n, \dots$ will have the property that $\tilde{x}_n = \beta_0$ for all $n.$ Likewise, the Aitken method is exact for sequences of the form $x_{n+1} = \beta_0 + \beta_1x_n,$ and so can be thought of as the deterministic analogue of an $AR(1)$ model.




The process of acceleration isn't quite so neat in practice because sequences don't adhere perfectly to these simple forms. Instead, the best that can be hoped for is that the  transformed sequence converges to the same limit as the original, but the rate of convergence is higher. For example, if transformation (\ref{eqn:rec_trans}) is applied to a sequence of the form $y_n = \beta_0 + \frac{\beta_1}{n} + \frac{\beta_2}{n^2},$ then the transformed sequence is now of the form $\tilde{y}_n = \beta_0 + \mathcal{O}(\frac{1}{n^2}),$ which converges to $\beta_0$ more quickly than the original sequence.\footnote{Doing the algebra, it can be seen that it is now the case that $\hat{\beta}_{1,n} =  \beta_1 + \beta_2 \left[\frac{2n + 1}{n(n+1)}\right],$ and so $\tilde{y}_n = \beta_0 +\beta_{2}\left[-\frac{2n + 1}{n^2(n+1)} + \frac{1}{n^2}\right] = \beta_0  + \beta_{2}\left[\frac{-n^3 - n^2  + n}{n^4(n+1)}\right] .$}


Suppose a convergent  sequence is of the form $x_{n+1} = f(x_{n})$ with $f(\cdot)$ differentiable and $x^*$ is the limit. Using a first order Taylor expansion, it can be seen that   for sufficiently large $n,$ $x_{n+1} \approx  x^* +  f'(x^*)(x_n - x^*).$ In this case, Aitken acceleration has a decent chance of accelerating the sequence so long as it has 'burned in' sufficiently.

One generalisation, proposed in \cite{isaacson2012analysis} is to use higher order polynomials to model the inverse error function $h(e).$ So $h(e)$ would be approximated by a quadratic through $(e_n, x_n), (e_{n-1}, x_{n-1})$ and $(e_{n-2}, x_{n-2}).$ Making $e$ the independent variable here instead of $x$ means the estimated limit can simply be found by evaluating the approximating quadratic at $e = 0$ instead of having to find the correct root of a quadratic to compute each element of the accelerated sequence.

Another approach is to simply apply Aitken Acceleration to the sequence twice. 

Both these approaches were attempted for the missing data model, and the results can be seen in Table \ref{tab:sse_error} and Figure \ref{fig:double_sse_error}. It can be seen that both methods improve convergence, though double Aitken acceleration is more effective (and easier to implement).

One can take the process further. For the missing values linear model, these higher-order methods converge very rapidly and are prone to numerical instability thereafter due to the error terms being so small. If the Aitken method is applied three times to the original sequence, the first entry yields the limit immediately and there is no need to go any further. Applying the quadratic method twice in a row produces a new sequence for which the first entry is within $10^{-12}$ of the limit.


\paragraph{Other Approaches:} There are alternative approaches besides those described here. For example, the EM and MM algorithms generate a sequence of coefficient vectors $\{\mathbf{c}_0, \mathbf{c}_1, \dots, \mathbf{c}_n, \dots \}$ with $\mathbf{c}_{n+1} = \mathbf{F}(\mathbf{c}_n)$ for some function $\mathbf{F}(\cdot).$ In our particular situation, the  function $\mathbf{F}(\cdot)$ would denote the operator that  takes a coefficient vector and returns the coefficient vector that minimises the associated $WPENSSE$ problem (\ref{eqn:wpensse}). The limit of this sequence - should it exist - is a solution to the equation $\mathbf{c} = \mathbf{F}(\mathbf{c}).$ It is  proposed in the literature to use Newton or Quasi-Newton methods  such as those described in the chapter on Brent's method to  numerically solve this fixed point equation \cite{chan2017acceleration, dempster1977maximum}. The idea is that such methods will find the fixed point more rapidly than simply iterating $\mathbf{F}(\cdot)$ until one gets sufficiently close to the limit. These methods have  the disadvantage of being more complex and time consuming to implement than the univariate acceleration methods.

\begin{figure}
\centering
   \includegraphics[height=9cm]{anova_accel.pdf}
   \caption{Log Errors for original sequnce of SSE values and the accelerated one}
   \label{fig:sse_error}
\end{figure}


\begin{figure}
\centering
   \includegraphics[height=9cm]{anova_double_accel.pdf}
   \caption{More sophisticated acceleration methods can provide a further boost to convergence. There are gaps in the plot because the more accelerated iterations have no valid log error  since  R cannot numerically distinguish them from the final limit.}
   \label{fig:double_sse_error}
\end{figure}
\newpage
\begin{table}[]

\caption{Iterations of the original sequence $SSE_n,$ the accelerated sequence $ASSE_n,$ the quadratically accelerated sequence $QASSE_n,$ and the doubly accelerated sequence $DASSE_n.$}

\label{tab:sse_error}
\centering
\begin{tabular}{ccccc}
     & $SSE_n$          & $ASSE_n$  & $QASSE_n$        & $DASSE_n$       \\
\hline
1     & 4949.6944444444 & 3203.8032711619 & 3203.7834325738 & 3203.7829457122 \\
2     & 3575.1658950617 & 3203.7843303225 & 3203.7829788622 & 3203.7829457359 \\
3     & 3282.7945625667 & 3203.7830400346 & 3203.7829479917 & 3203.7829457364 \\
4     & 3220.5935028609 & 3203.7829521582 & 3203.7829458900 & 3203.7829457364 \\
5     & 3207.3596279977 & 3203.7829461738 & 3203.7829457469 & 3203.7829457364 \\
6     & 3204.5439391293 & 3203.7829457662 & 3203.7829457371 & 3203.7829457364 \\
7     & 3203.9448588925 & 3203.7829457385 & 3203.7829457365 & 3203.7829457364 \\
8     & 3203.8173952920 & 3203.7829457366 & 3203.7829457364 & 3203.7829457364 \\
9     & 3203.7902754193 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
10    & 3203.7845052414 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
11    & 3203.7832775456 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
12    & 3203.7830163340 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
13    & 3203.7829607572 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
14    & 3203.7829489323 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
15    & 3203.7829464164 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
16    & 3203.7829458811 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
17    & 3203.7829457672 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
18    & 3203.7829457430 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
19    & 3203.7829457378 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
20    & 3203.7829457367 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
21    & 3203.7829457365 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
22    & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
\hline
$\infty$ & 3203.7829457364 & 3203.7829457364 &  3203.7829457364 & 3203.7829457364
\end{tabular}
\end{table}

\newpage
\clearpage
\subsection{Accelerating the $L_1$ Fitting Alogrithm}

The $L_1$ fitting algorithm devised in Section \ref{sec:mm_l1_problem} is much more difficult to accelerate as can be seen in Figures \ref{fig:sae_accel} and \ref{fig:sae_multi_accel}. Figure \ref{fig:sae_accel}  shows the result of applying Aitken acceleration to the $SAE$ sequence generated, and Figure \ref{fig:sae_multi_accel} shows the results of attempting an entire battery of acceleration methods, including methods specifically designed for accelerating slowly converging sequences that Aitken acceleration cannot accelerate such as Epsilon Algorithm and Lubkin's W transform. \cite{graves2000epsilon, osada1993acceleration,wimp1981sequence} No method performs substantially better than Aitken acceleration. The $SAE$ sequence is apparently either numerically ill-behaved or of a very unusual form.

To conclude, it is possible to save some time by acceleration, but the scope for doing so is limited and the process would have to be monitored carefully to ensure that numerical instability isn't causing trouble. 


\begin{figure}
\centering
   \includegraphics[height=9cm]{l1_accleration_plot.pdf}
   \caption{Accelerating the SAE sequence generating by the $L_1$ fitting alogrithm using Aitken's Method. The improvement in covergence is  mediocre.}
\label{fig:sae_accel}
\end{figure}

\begin{figure}
\centering
   \includegraphics[height=9cm]{multi_accleration_plot.pdf}
   \caption{Accelerating the SAE sequence using multiple methods. Aitken's method performs the best, despite it's lack of sophistication.}
\label{fig:sae_multi_accel}
\end{figure}


