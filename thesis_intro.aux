\relax 
\citation{ramsay2005functional}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Preliminaries}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Functional Data Analysis}{4}}
\@writefile{toc}{\contentsline {subsubsection}{Specification of Function Spaces}{4}}
\citation{kloeden2013numerical}
\citation{iacus2009simulation}
\citation{ramsay2009functional,ramsay2005functional}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Statistcal Modelling Process For Functions\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statModellingProcess}{{1.1}{6}}
\citation{iacus2009simulation}
\citation{kelly2002filtering}
\@writefile{toc}{\contentsline {paragraph}{Formulate a Model:}{7}}
\@writefile{toc}{\contentsline {paragraph}{Construct a Discretised Model that Approximates the Original Model:}{7}}
\@writefile{toc}{\contentsline {paragraph}{Conduct Statistical Analysis Using the Discretised Model:}{7}}
\@writefile{toc}{\contentsline {paragraph}{Check the Approximation Error in Discretised Model:}{7}}
\@writefile{toc}{\contentsline {paragraph}{Check If Results of Statistical Analysis Are Consistent With Discretised Model.}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Exact Penalised Regression}{9}}
\newlabel{sec:pen_regression}{{1.2}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Smoothing Splines}{9}}
\newlabel{sec:smoothing_splines}{{1.2.1}{9}}
\newlabel{eqn:penalised_second_deriv}{{1.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Piecewise Trigonometric Interpolation}{10}}
\newlabel{sec:trignometric_interpolation}{{1.2.2}{10}}
\newlabel{eqn:penalised_trig}{{1.2}{10}}
\citation{schumaker2007spline}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Plot of a Piecewise Trigonometric Curve. Note the kinks between segments.\relax }}{11}}
\newlabel{fig:pieceTrigCurve}{{1.2}{11}}
\citation{boyd2001chebyshev}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Penalised Regression Using Finite Dimensional Approximations}{12}}
\newlabel{sec:finite_dimension_general}{{1.3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}FDA With a Quadratic Basis}{12}}
\newlabel{sec:fda_quadratic_basis}{{1.3.1}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Statistical Modelling Process For Functional Data Analysis\relax }}{13}}
\newlabel{fig:fda_modelling}{{1.3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Elements of Functional Data Analysis\relax }}{14}}
\newlabel{fig:fda_modelling_cyclical}{{1.4}{14}}
\newlabel{eqn:pen_quad}{{1.3}{15}}
\citation{isaacson2012analysis,teschl2012ordinary}
\citation{fdapackage,ramsay2009functional}
\citation{ramsay2005functional}
\citation{fdapackage}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Performing FDA with the differential operator $Lf = t^2f'' - 0.5f$ and the basis set $\{1, t, t^2\}.$\relax }}{17}}
\newlabel{fig:quad_model}{{1.5}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The \texttt  {FDA} Package}{17}}
\newlabel{sec:intro_fda_package}{{1.4}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Using the \texttt  {FDA} package to smooth the melanoma data with the differential operator $Lf = f - \omega ^2f^{(4)}.$\relax }}{19}}
\newlabel{fig:fda_mela_smooth}{{1.6}{19}}
\citation{ramsay2005functional}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}The \texttt  {Data2LD} Package}{20}}
\newlabel{sec:intro_data2ld}{{1.5}{20}}
\newlabel{fig:ode_system}{{1.6}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Parametric estimation of ODE parameters versus \texttt  {Data2LD}}{20}}
\newlabel{sec:reflux_parametric_vs_data2ld}{{1.6}{20}}
\newlabel{eqn:reflux_ode}{{1.7}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Fitting the Data Parametrically by Solving the ODE Model}{21}}
\newlabel{sec:reflux_parametric_approach}{{1.6.1}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Parametric Fitting}{22}}
\newlabel{eqn:reflux_model}{{1.8}{22}}
\@writefile{toc}{\contentsline {paragraph}{Estimating $\beta _0$ from the data:}{22}}
\@writefile{toc}{\contentsline {paragraph}{Estimating $\beta _1$ and $\beta _2$ from $\beta _0$ and the data:}{22}}
\newlabel{eqn:reflux_rearranged_model}{{1.9}{22}}
\@writefile{toc}{\contentsline {paragraph}{Simultaneous Estimation of Parameters:}{22}}
\@writefile{toc}{\contentsline {paragraph}{Matching:}{22}}
\@writefile{toc}{\contentsline {paragraph}{Breakpoint Estimation:}{23}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Fitting the Reflux Data with \texttt  {Data2LD}}{23}}
\newlabel{sec:reflux_data2ld_fit}{{1.6.2}{23}}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Rates Of Convergence}{24}}
\newlabel{sec:rates_of_convergence}{{1.7}{24}}
\@writefile{toc}{\contentsline {paragraph}{Linear Convergence:}{24}}
\newlabel{eqn:linear_convergence}{{1.10}{24}}
\@writefile{toc}{\contentsline {paragraph}{Sublinear Convergence:}{24}}
\@writefile{toc}{\contentsline {paragraph}{Superlinear and Quadratic Convergence:}{24}}
\citation{nocedalnumerical}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Illustrating the different classes of convergence.\relax }}{25}}
\newlabel{eqn:superlinear_convergence}{{1.11}{25}}
\@writefile{toc}{\contentsline {paragraph}{An Extended Definition of Convergence Rates:}{25}}
\@writefile{toc}{\contentsline {paragraph}{Linear Convergence and Iterated Mappings:}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Overview of Appendices}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Reflux Data\relax }}{28}}
\newlabel{fig:refluxPlot}{{1.7}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces The fitted curve is constructed by combining two functions together using the maximum operator.\relax }}{29}}
\newlabel{fig:constituentMaxFunctions}{{1.8}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Plot of the parameteric fit to the the Reflux data.\relax }}{30}}
\newlabel{fig:reflux_ode_fit}{{1.9}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Modelling the Reflux data using \texttt  {Data2LD}.\relax }}{31}}
\newlabel{fig:reflux_fda_fit}{{1.10}{31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Hierarchical Estimation and the Parameter Cascade}{32}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:paramcascade}{{2}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Fitting the Reflux Data Using a Semi-Parametric ODE Model}{32}}
\citation{ramsay2005functional}
\citation{cao2007parameter}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A solution to the general Reflux ODE with a constant plus sinusoidal forcing function.\relax }}{35}}
\newlabel{fig:reflux_ode_arbitary}{{2.1}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Two-Stage Parameter Cascade}{35}}
\citation{cao2007parameter,ramsay2007parameter}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The Three-Stage Parameter Cascade}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Two Stage Parameter Cascade (Simplified)\relax }}{37}}
\newlabel{fig:twoStageParamSimplified}{{2.2}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Schematic of the Two Stage Parameter Cascade With the Inner Optimisation Visible\relax }}{38}}
\newlabel{fig:twoStageParam}{{2.3}{38}}
\citation{data2ld}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Investigating the \texttt  {Data2LD} Package}{39}}
\newlabel{sec:data2ld_investigation}{{2.4}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}How \texttt  {Data2LD} Estimates Parameters}{39}}
\newlabel{eqn:grad_descent}{{{\textbf  {S1}}}{39}}
\newlabel{eqn:newton_method}{{{\textbf  {S2}}}{39}}
\citation{nocedalnumerical}
\newlabel{eqn:t1}{{{\textbf  {T1}}}{40}}
\newlabel{eqn:t2}{{{\textbf  {T2}}}{40}}
\newlabel{eqn:t3}{{{\textbf  {T3}}}{40}}
\newlabel{eqn:t4}{{{\textbf  {T4}}}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Point A is the initial point. Point B passes {\textbf  {T1}}\hbox {} with $c_1 = 0.5$ and passes {\textbf  {T2}}\hbox {} with $c_2 = 0.9.$ Point C fails {\textbf  {T1}}\hbox {} with $c_1 = 0.5$ and also fails {\textbf  {T3}}\hbox {}, but passes {\textbf  {T4}}\hbox {}, and passes {\textbf  {T2}}\hbox {} with $c_2 = 0.9.$ Point D fails all four tests.\relax }}{41}}
\newlabel{fig:data2ld_line_search_tests}{{2.4}{41}}
\citation{mcconnell2004code}
\citation{strauss2007partial}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Hierarchical fitting of a Partial Differential Equation}{42}}
\newlabel{sec:pde_fitting_strategy}{{2.5}{42}}
\citation{cao2007parameter}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Derivative-Free Optimisation and the Parameter Cascade}{44}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:brent}{{3}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview of Quadratic Optimisation Methods}{44}}
\newlabel{sec:quad_methods}{{3.1}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Newton's Method}{44}}
\citation{nocedalnumerical,isaacson2012analysis,lange2004optimization,lange2010numerical}
\citation{isaacson2012analysis}
\citation{isaacson2012analysis,nocedalnumerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Secant Method}{45}}
\newlabel{eqn:seq_finite_diff}{{3.1}{45}}
\citation{isaacson2012analysis}
\citation{nocedalnumerical,vandebogart2017method}
\citation{vandebogart2017method}
\citation{nocedalnumerical}
\citation{nocedalnumerical,isaacson2012analysis,lange2010numerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Successive Parabolic Interpolation}{46}}
\citation{data2ld}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Discussion}{47}}
\newlabel{sec:quad_discussion}{{3.1.4}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Modifying the \texttt  {Data2LD.opt} Routine}{47}}
\@writefile{toc}{\contentsline {paragraph}{Approximating the Derivative:}{47}}
\newlabel{eqn:secant_diff_approx}{{3.2}{48}}
\newlabel{eqn:data_2ld_secant}{{3.3}{48}}
\@writefile{toc}{\contentsline {paragraph}{Comparing the Modfied Method with the Original Method:}{48}}
\@writefile{toc}{\contentsline {paragraph}{Newton Method with Gradient Line Search (Data2LD.opt)}{48}}
\citation{lange2010numerical,chong2013introduction}
\@writefile{toc}{\contentsline {paragraph}{Newton Method with Secant Approximation To First Derivative (Modified)}{49}}
\citation{brent2013algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Golden-Section Search}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Golden-Section search. If $f(c) < f(d),$ the next triplet is given by $\{a,c,d\},$ otherwise $\{c, d, b\}$ is used.\relax }}{50}}
\newlabel{fig:golden_section}{{3.1}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Brent's Method}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Estimation Of Parameters For a Standard Cauchy Distribution Using Brent's Method}{51}}
\newlabel{sec:cauchy_estimation}{{3.5}{51}}
\citation{pawitan2001all}
\citation{leveque2007finite,fornberg1988generation}
\citation{small1990survey}
\citation{desolve}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Robust ODE Parameter Estimation}{53}}
\newlabel{cauchy_ode}{{3.6}{53}}
\newlabel{eqn:nonlinear_ode}{{3.4}{53}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The Parameter Cascade and Brent's Method}{55}}
\newlabel{brent_param}{{3.7}{55}}
\newlabel{eqn:outer_objective_function}{{3.5}{55}}
\citation{cao2007parameter}
\@writefile{toc}{\contentsline {subsubsection}{Melanoma Data}{56}}
\newlabel{eqn:param_operator}{{3.6}{56}}
\citation{shor2012minimization}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Profile log likelihood in $\sigma ,$ and contour plot of the joint log likelihood. \relax }}{58}}
\newlabel{fig:cauchy_likelihood}{{3.2}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Plot of profile score statistic.\relax }}{59}}
\newlabel{fig:profile_score_statistic}{{3.3}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Top: Displays the SAE objective function; Bottom: Displays the true curve, the generated data corrupted with Cauchy noise, and the resulting fitted curve using Brent's method.\relax }}{60}}
\newlabel{fig:cauchy_ode}{{3.4}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Plots of the middle and outer optimisation problems.\relax }}{61}}
\newlabel{fig:mela_omega}{{3.5}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Plot of the middle optimisation problem with MAD used as a loss function\relax }}{62}}
\newlabel{fig:mela_mad_omega}{{3.6}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Comparison of fits for MAD and SSE criteria for middle problem\relax }}{62}}
\newlabel{fig:mela_l2_mad_omega}{{3.7}{62}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}A Two-Stage $L_1$ Parameter Cascade Using the MM Algorithm}{63}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:mm_methods}{{4}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}$L_1$ Estimation for the Inner Problem}{63}}
\newlabel{sec:mm_l1_problem}{{4.1}{63}}
\citation{hunter2004tutorial,lange2004optimization,lange2010numerical}
\citation{small1990survey}
\citation{hunter2004tutorial,lange2004optimization,lange2010numerical}
\citation{wu2010mm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}An MM Algorithm For Computing the Median}{64}}
\citation{lange2007slides,hunter2004tutorial}
\citation{lange2007slides}
\citation{rudin1964principles}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Penalised $L_1$ Fitting}{66}}
\newlabel{sec:mm_l1_algorithm}{{4.1.2}{66}}
\newlabel{eqn:wpensse}{{4.1}{66}}
\citation{mclachlan2007algorithm}
\citation{wu2010mm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Discussion}{67}}
\newlabel{sec:discuss}{{4.1.3}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Testing the Algorithm on the Melanoma Data}{67}}
\newlabel{sec:testing}{{4.1.4}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison of $L_1$ and $L_2$ inner fits to Cauchy-perturbed data with $\omega $ fixed at 0.3\relax }}{68}}
\newlabel{fig:mela_l1_l2}{{4.1}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Plot of values and log differences for SAE Statistic\relax }}{69}}
\newlabel{fig:sae_plot}{{4.2}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Plot of PENSAE statistic as the algorithm proceeds.\relax }}{70}}
\newlabel{fig:pensae_plot}{{4.3}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Plot of log norm differences for coefficients. Note that they tend to settle on a line.\relax }}{70}}
\newlabel{fig:coef_plot}{{4.4}{70}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Two-Stage Parameter Cascade with $L_1$ Estimation at Inner and Middle Levels}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Fitting an $L_1$ Parameter Cascade to the Melanoma Data\relax }}{72}}
\newlabel{fig:mela_l1_cascade}{{4.5}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces $L_1$ and $L_2$ Parameter Cascades with the same perturbed data as in Figure 4.1\hbox {}. Compare the $L_1$ curve in this plot with the one in Figure 4.5\hbox {}.\relax }}{73}}
\newlabel{fig:mela_l1_l2_cascade}{{4.6}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces All possible combinations of $L_1$ and $L_2$ loss functions that can be used for the Parameter Cascade. The top plot applies them to the original melanoma data, the bottom to the same perturbed data as in Figures 4.1\hbox {} and 4.6\hbox {}\relax }}{74}}
\newlabel{fig:mela_l1_l2_combiations}{{4.7}{74}}
\citation{mclachlan2007algorithm}
\citation{mclachlan2007algorithm}
\citation{wu2010mm}
\citation{dempster1977maximum,mclachlan2007algorithm}
\citation{isaacson2012analysis}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Accelerating The Rate of Convergence}{75}}
\citation{mclachlan2007algorithm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Example: Using Imputation to Fit an ANOVA Model With Missing Data}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Generalisations}{76}}
\newlabel{eqn:rec_seq}{{4.3}{76}}
\citation{isaacson2012analysis}
\citation{chan2017acceleration,dempster1977maximum}
\newlabel{eqn:rec_trans}{{4.4}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Log Errors for original sequence of SSE values and the accelerated one\relax }}{78}}
\newlabel{fig:sse_error}{{4.8}{78}}
\@writefile{toc}{\contentsline {paragraph}{Other Approaches:}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces More sophisticated acceleration methods can provide a further boost to convergence. There are gaps in the plot because the more accelerated iterations have no valid log error since R cannot numerically distinguish them from the final limit.\relax }}{79}}
\newlabel{fig:double_sse_error}{{4.9}{79}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Iterations of the original sequence $SSE_n,$ the accelerated sequence $ASSE_n,$ the quadratically accelerated sequence $QASSE_n,$ and the doubly accelerated sequence $DASSE_n.$\relax }}{80}}
\newlabel{tab:sse_error}{{4.1}{80}}
\citation{graves2000epsilon,osada1993acceleration,wimp1981sequence}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Accelerating the SAE sequence generating by the $L_1$ fitting algorithm using Aitken's Method. The improvement in covergence is mediocre.\relax }}{81}}
\newlabel{fig:sae_accel}{{4.10}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Accelerating the $L_1$ Fitting Algorithm}{81}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Accelerating the SAE sequence using multiple methods. Aitken's method performs the best, despite it's lack of sophistication.\relax }}{82}}
\newlabel{fig:sae_multi_accel}{{4.11}{82}}
\citation{ramsay2017dynamic,hooker2016collocinfer}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Profiling Non-Linear Regression Models With the Parameter Cascade}{83}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:nonlinear}{{5}{83}}
\citation{hydon2000symmetry}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Fitting a Non-Linear Regression Model With the Parameter Cascade}{84}}
\newlabel{sec:param_cascade_nonlinear_regress}{{5.1}{84}}
\newlabel{eqn:nonlinear_model}{{5.1}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Fitting Linear Homogeneous ODEs Using the Parameter Cascade}{85}}
\newlabel{eqn:ode_model}{{5.2}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Estimation for First Order Linear PDEs}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}The Transport Equation}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Profile Plot and Fitted Curve\relax }}{88}}
\newlabel{fig:non_linear_model}{{5.1}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Plot of fit to simulated data, and contour plot of SSE against $\alpha $ and $\beta .$ Blue dot is true parameter values, red is estimated parameter values.\relax }}{89}}
\newlabel{fig:ode_plot}{{5.2}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Plot of middle optimisation for the transport equation. Blue line denotes original parameter, red line denotes fitted estimate\relax }}{90}}
\newlabel{fig:transport_profile}{{5.3}{90}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The estimates of $f(x)$ computed for various sample sizes. The plot (a) gives the result with 200 sample points, the plot (b) gives the result for 2000 sample points. The variable $z = x - \beta t, z$ was used as the independent variable to avoid confusion between $x$ and $x - \beta t.$ Furthermore, the points used to fit $f(x)$ aren't displayed to reduce clutter and make it easier to compare the fitted curves with the original.\relax }}{91}}
\newlabel{fig:transport_equation_fit}{{5.4}{91}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{91}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{91}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Plot of outer optimisation for the modified transport equation with only 30 sample points. Blue line denotes original parameter, red line denotes fitted estimate\relax }}{92}}
\newlabel{fig:mod_transport_profile}{{5.5}{92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}The Transport Equation with Space-varying Velocity}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Plotted estimate of $f(x)$ for the modified transport equation with 30 sample points.\relax }}{93}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and Further Research}{94}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{94}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Derivative-Free versus Derivative-Based Methods}{94}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Further Research}{94}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Quasi-linear Differential Problems}{95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Discussion}{95}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{96}}
\citation{nocedalnumerical,chong2013introduction}
\citation{nocedalnumerical}
\citation{chong2013introduction}
\citation{nocedalnumerical,chong2013introduction,mathematicsdata2018}
\citation{mathematicsdata2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Overview of Optimisation Methods}{97}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:optimisation_methods}{{A}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Gradient Descent and the Newton-Raphson method}{97}}
\newlabel{sec:newton_gradient}{{A.1}{97}}
\citation{nocedalnumerical,isaacson2012analysis,lange2010numerical,lange2004optimization,chong2013introduction}
\citation{isaacson2012analysis}
\citation{isaacson2012analysis}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Plot of the log errors from applying Gradient Descent to the function $f(x) = x^4.$\relax }}{99}}
\newlabel{fig:grad_descent}{{A.1}{99}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Plot of the log errors from applying Gradient Descent and the Newton-Raphson Method to the function $f(x) = x^4.$\relax }}{100}}
\newlabel{fig:newton_grad_descent}{{A.2}{100}}
\citation{kelley1995iterative}
\citation{kelley1995iterative}
\citation{isaacson2012analysis}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}The Chord Method}{101}}
\newlabel{sec:chord_methods}{{A.2}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}More Sophisticated Hessian Approximations}{101}}
\newlabel{sec:higher_order_methods}{{A.3}{101}}
\citation{nocedalnumerical,lange2004optimization,lange2010numerical}
\citation{nocedalnumerical}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Linearly Convergent Methods}{102}}
\citation{kelley1995iterative}
\citation{rlanguage}
\citation{rlanguage,nocedalnumerical}
\citation{nocedalnumerical}
\citation{nocedalnumerical,leveque2007finite,kelley1995iterative}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Quasi-Newton Methods}{103}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Finite Differences}{103}}
\citation{kelley1995iterative}
\newlabel{eqn:finite_diff_error}{{A.1}{104}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Plot of the log iterates produced from the recurrence relation $e_{n+1} = \mathaccentV {bar}016{e} + (e_n + h)e_n$ with $e_0 = 0.5,$ for various choices of $\mathaccentV {bar}016{e}$ and $h.$\relax }}{105}}
\newlabel{fig:finite_difference_converge}{{A.3}{105}}
\@writefile{toc}{\contentsline {section}{\numberline {A.7}Line Search Methods}{105}}
\newlabel{sec:line_search_methods}{{A.7}{105}}
\citation{nocedalnumerical}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Extrapolating too far out can lead to disaster!\relax }}{106}}
\newlabel{fig:extrapolateDisaster}{{A.4}{106}}
\citation{nocedalnumerical}
\citation{tamir1976line}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sketch of Backtracking Line Search\relax }}{107}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7.1}Wolfe Conditions}{107}}
\citation{barzilai1982nonpolynomial}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7.2}More sophisticated Line Search Algorithms}{108}}
\citation{nocedalnumerical,kelly2002filtering,kelley2011implicit}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Implicit Filtering}{109}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:implicit_filtering}{{B}{109}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Description of the Algorithm}{109}}
\citation{kelley2001implicit}
\citation{nocedalnumerical,kelly2002filtering,kelley2011implicit,kelley2001implicit}
\newlabel{eqn:forward_difference}{{B.1}{110}}
\newlabel{eqn:implicit_filtering_convergence_criterion}{{B.2}{110}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Using Implicit Filtering to Fit an ODE to the Melanoma Data}{111}}
\newlabel{sec:implicit_brent}{{B.2}{111}}
\newlabel{eqn:quasi_linear_oscillator}{{B.3}{111}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Time taken for Implicit Filtering to fit (B.3\hbox {}) to the melanoma data for various values of $\delta .$\relax }}{112}}
\newlabel{tab:run_times}{{B.1}{112}}
\citation{*}
\bibstyle{plain}
\bibdata{ref}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Fitting the ODE (B.3\hbox {}) to the Melanoma data. The exact value of the shrink value $\delta $ affects the fit the Implicit Filtering algorithm converges to. For $\delta = 0.7$ the computed fit in (a) resembles a straight line, but $\delta = 0.9$ results in a sinusoidal plus linear trend as can be seen in (b).\relax }}{113}}
\newlabel{fig:implicit_filtering}{{B.1}{113}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{113}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{113}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Without a penalty term, Implicit Filtering entirely fails to fit the ODE (B.3\hbox {}) to the melanoma data.\relax }}{114}}
\newlabel{fig:implicit_filtering_no_penalty}{{B.2}{114}}
