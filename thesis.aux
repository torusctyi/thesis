\relax 
\citation{ramsay2005functional}
\citation{schilling2017measures}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Preliminaries}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Functional Data Analysis}{4}}
\@writefile{toc}{\contentsline {subsubsection}{Specification of Function Spaces}{4}}
\citation{kloeden2013numerical}
\citation{iacus2009simulation}
\citation{ramsay2009functional,ramsay2005functional}
\citation{iacus2009simulation}
\@writefile{toc}{\contentsline {paragraph}{Formulate a Model:}{6}}
\@writefile{toc}{\contentsline {paragraph}{Construct a Discretised Model that Approximates the Original Model:}{6}}
\@writefile{toc}{\contentsline {paragraph}{Conduct Statistical Analysis Using the Discretised Model:}{6}}
\@writefile{toc}{\contentsline {paragraph}{Check the Approximation Error in Discretised Model:}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Statistcal Modelling Process For Functions\relax }}{7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statModellingProcess}{{1.1}{7}}
\citation{kelly2002filtering}
\@writefile{toc}{\contentsline {paragraph}{Check If Results of Statistical Analysis Are Consistent With Discretised Model.}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Exact Penalised Regression}{9}}
\newlabel{sec:pen_regression}{{1.2}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Smoothing Splines}{9}}
\newlabel{sec:smoothing_splines}{{1.2.1}{9}}
\newlabel{eqn:penalised_second_deriv}{{1.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Piecewise Trigonometric Interpolation}{10}}
\newlabel{sec:trignometric_interpolation}{{1.2.2}{10}}
\citation{schumaker2007spline}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Plot of a Piecewise Trigonometric Curve. Note the kinks between segments.\relax }}{11}}
\newlabel{fig:pieceTrigCurve}{{1.2}{11}}
\citation{boyd2001chebyshev}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Penalised Regression Using Finite Dimensional Approximations}{12}}
\newlabel{sec:finite_dimension_general}{{1.3}{12}}
\citation{boyd2001chebyshev}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}FDA With a Quadratic Basis}{13}}
\newlabel{sec:fda_quadratic_basis}{{1.3.1}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Statistical Modelling Process For Functional Data Analysis\relax }}{14}}
\newlabel{fig:fda_modelling}{{1.3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Elements of Functional Data Analysis\relax }}{15}}
\newlabel{fig:fda_modelling_cyclical}{{1.4}{15}}
\citation{isaacson2012analysis,teschl2012ordinary}
\newlabel{eqn:pen_quad}{{1.2}{16}}
\citation{fdapackage,ramsay2009functional}
\citation{ramsay2005functional}
\citation{fdapackage}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Performing FDA with the differential operator $Lf = t^2f'' - 0.5f$ and the basis set $\{1, t, t^2\}.$\relax }}{17}}
\newlabel{fig:quad_model}{{1.5}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The \texttt  {FDA} Package}{17}}
\newlabel{sec:intro_fda_package}{{1.4}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Using the \texttt  {FDA} package to smooth the melanoma data with the differential operator $Lf = f - \omega ^2f^{(4)}.$\relax }}{18}}
\newlabel{fig:fda_mela_smooth}{{1.6}{18}}
\citation{ramsay2005functional}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}The \texttt  {Data2LD} Package}{19}}
\newlabel{sec:intro_data2ld}{{1.5}{19}}
\newlabel{fig:ode_system}{{1.5}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Modelling the Reflux Data: Parametric Approaches and \texttt  {Data2LD}}{19}}
\newlabel{sec:reflux_parametric_vs_data2ld}{{1.6}{19}}
\newlabel{eqn:reflux_ode}{{1.6}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Reflux Data\relax }}{20}}
\newlabel{fig:refluxPlot}{{1.7}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Fitting the Reflux Data Parametrically by Solving the ODE Model}{21}}
\newlabel{sec:reflux_parametric_approach}{{1.6.1}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces The fitted curve is constructed by combining two functions together using the maximum operator.\relax }}{22}}
\newlabel{fig:constituentMaxFunctions}{{1.8}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Parametric Fitting}{23}}
\newlabel{eqn:reflux_model}{{1.7}{23}}
\@writefile{toc}{\contentsline {paragraph}{Estimating $\beta _0$ from the data:}{23}}
\@writefile{toc}{\contentsline {paragraph}{Estimating $\beta _1$ and $\beta _2$ from $\beta _0$ and the data:}{23}}
\newlabel{eqn:reflux_rearranged_model}{{1.8}{23}}
\@writefile{toc}{\contentsline {paragraph}{Simulataneous Estimation of Parameters:}{23}}
\@writefile{toc}{\contentsline {paragraph}{Matching:}{23}}
\citation{iacus2009simulation}
\@writefile{toc}{\contentsline {paragraph}{Breakpoint Estimation:}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Plot of the parameteric fit to the the Reflux data.\relax }}{25}}
\newlabel{fig:reflux_ode_fit}{{1.9}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Fitting the Reflux Data Parametrically Using a Collocation Method}{26}}
\newlabel{sec:reflux_collocation_fit}{{1.6.2}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Smoothing the Melanoma data using a finite difference approximation does not produce a particularly smooth fit.\relax }}{28}}
\newlabel{fig:mela_finite_difference_smooth}{{1.10}{28}}
\citation{nocedalnumerical,chong2013introduction}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Fitting the Reflux Data with \texttt  {Data2LD}}{29}}
\newlabel{sec:reflux_data2ld_fit}{{1.6.3}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Rates Of Convergence}{29}}
\newlabel{sec:rates_of_convergence}{{1.7}{29}}
\@writefile{toc}{\contentsline {paragraph}{Linear Convergence:}{29}}
\newlabel{eqn:linear_convergence}{{1.18}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Modelling the Reflux data using \texttt  {Data2LD}.\relax }}{30}}
\newlabel{fig:reflux_fda_fit}{{1.11}{30}}
\@writefile{toc}{\contentsline {paragraph}{Sublinear Convergence:}{31}}
\@writefile{toc}{\contentsline {paragraph}{Superlinear and Quadratic Convergence:}{31}}
\newlabel{eqn:superlinear_convergence}{{1.19}{31}}
\citation{nocedalnumerical}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Illustrating the different classes of convergence.\relax }}{32}}
\@writefile{toc}{\contentsline {paragraph}{An Extended Definition of Convergence Rates:}{32}}
\@writefile{toc}{\contentsline {paragraph}{Linear Convergence and Iterated Mappings:}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Overview of Appendices}{33}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Hierarchical Estimation and the Parameter Cascade}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Fitting the Reflux Data Using a Semi-Parametric ODE Model}{34}}
\citation{ramsay2005functional}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A solution to the general Reflux ODE with a constant plus sinusoidal forcing function.\relax }}{37}}
\newlabel{fig:reflux_ode_arbitary}{{2.1}{37}}
\citation{cao2007parameter}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Two-Stage Parameter Cascade}{38}}
\citation{cao2007parameter,ramsay2007parameter}
\citation{data2ld}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The Three-Stage Parameter Cascade}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Investigating the \texttt  {Data2LD} Package}{39}}
\newlabel{sec:data2ld_investigation}{{2.4}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Two Stage Parameter Cascade (Simplified)\relax }}{40}}
\newlabel{fig:twoStageParamSimplified}{{2.2}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Schematic of the Two Stage Parameter Cascade With the Inner Optimisation Visible\relax }}{41}}
\newlabel{fig:twoStageParam}{{2.3}{41}}
\citation{mcconnell2004code}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}How \texttt  {Data2LD} Estimates Parameters}{42}}
\newlabel{eqn:grad_descent}{{{\textbf  {S1}}}{42}}
\newlabel{eqn:newton_method}{{{\textbf  {S2}}}{42}}
\newlabel{eqn:t1}{{{\textbf  {T1}}}{42}}
\newlabel{eqn:t2}{{{\textbf  {T2}}}{42}}
\newlabel{eqn:t3}{{{\textbf  {T3}}}{42}}
\citation{nocedalnumerical}
\newlabel{eqn:t4}{{{\textbf  {T4}}}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Point A is the initial point. Point B passes {\textbf  {T1}}\hbox {} with $c_1 = 0.5$ and passes {\textbf  {T2}}\hbox {} with $c_2 = 0.9.$ Point C fails {\textbf  {T1}}\hbox {} with $c_1 = 0.5$ and also fails {\textbf  {T3}}\hbox {}, but passes {\textbf  {T4}}\hbox {}, and passes {\textbf  {T2}}\hbox {} with $c_2 = 0.9.$ Point D fails all four tests.\relax }}{44}}
\newlabel{fig:data2ld_line_search_tests}{{2.4}{44}}
\citation{strauss2007partial}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Hierarchical fitting of a Partial Differential Equation}{45}}
\newlabel{sec:pde_fitting_strategy}{{2.5}{45}}
\citation{cao2007parameter}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Derivative-Free Optimisation and the Parameter Cascade}{47}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:brent}{{3}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview of Quadratic Optimisation Methods}{47}}
\newlabel{sec:quad_methods}{{3.1}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Newton's Method}{47}}
\citation{nocedalnumerical,isaacson2012analysis,lange2004optimization,lange2010numerical}
\citation{isaacson2012analysis}
\citation{isaacson2012analysis,nocedalnumerical}
\citation{isaacson2012analysis}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Secant Method}{48}}
\newlabel{eqn:seq_finite_diff}{{3.1}{48}}
\citation{isaacson2012analysis}
\citation{nocedalnumerical,vandebogart2017method}
\citation{vandebogart2017method}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Successive Parabolic Interpolation}{49}}
\citation{nocedalnumerical,isaacson2012analysis,lange2010numerical}
\citation{data2ld}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Discussion}{50}}
\newlabel{sec:quad_discussion}{{3.1.4}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Modifying the \texttt  {Data2LD.opt} Routine}{50}}
\citation{kelley1995iterative}
\@writefile{toc}{\contentsline {paragraph}{Approximating the Derivative:}{51}}
\newlabel{eqn:secant_diff_approx}{{3.2}{51}}
\newlabel{eqn:data_2ld_secant}{{3.3}{51}}
\@writefile{toc}{\contentsline {paragraph}{Error Analysis:}{51}}
\newlabel{eqn:convergence_rate_error_in_deriv}{{3.4}{51}}
\citation{vandebogart2017method}
\@writefile{toc}{\contentsline {paragraph}{Comparison with the Secant Method and Successive Parabolic Interpolation:}{52}}
\@writefile{toc}{\contentsline {paragraph}{Comparing the Modfied Method with the Original Method:}{52}}
\@writefile{toc}{\contentsline {paragraph}{Newton Method with Gradient Line Search}{53}}
\@writefile{toc}{\contentsline {paragraph}{Newton Method with Secant Approximation To First Derivative}{53}}
\citation{lange2010numerical,chong2013introduction}
\citation{brent2013algorithms}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Golden-Section Search}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Brent's Method}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Golden-Section search. If $f(c) < f(d),$ the next triplet is given by $\{a,c,d\},$ otherwise $\{c, d, b\}$ is used.\relax }}{55}}
\newlabel{fig:golden_section}{{3.1}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Estimation Of Parameters For a Standard Cauchy Distribution Using Brent's Method}{55}}
\newlabel{sec:cauchy_estimation}{{3.5}{55}}
\citation{pawitan2001all}
\citation{leveque2007finite,fornberg1988generation}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Profile log likelihood in $\sigma ,$ and contour plot of the joint log likelihood. \relax }}{58}}
\newlabel{fig:cauchy_likelihood}{{3.2}{58}}
\citation{small1990survey}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Plot of profile score statistic.\relax }}{59}}
\newlabel{fig:profile_score_statistic}{{3.3}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Robust ODE Parameter Estimation}{59}}
\newlabel{cauchy_ode}{{3.6}{59}}
\citation{desolve}
\newlabel{eqn:nonlinear_ode}{{3.5}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Original curve, fitted curve, and objective function.\relax }}{61}}
\newlabel{fig:cauchy_ode}{{3.4}{61}}
\citation{cao2007parameter}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The Parameter Cascade and Brent's Method}{62}}
\newlabel{brent_param}{{3.7}{62}}
\newlabel{eqn:outer_objective_function}{{3.6}{62}}
\@writefile{toc}{\contentsline {subsubsection}{Melanoma Data}{63}}
\newlabel{eqn:param_operator}{{3.7}{63}}
\citation{shor2012minimization}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Plots of the middle and outer optimisation problems.\relax }}{65}}
\newlabel{fig:mela_omega}{{3.5}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Plot of the middle optimisation problem with MAD used as a loss function\relax }}{66}}
\newlabel{fig:mela_mad_omega}{{3.6}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Comparison of fits for MAD and SSE criteria for middle problem\relax }}{66}}
\newlabel{fig:mela_l2_mad_omega}{{3.7}{66}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}A Two-Stage $L_1$ Parameter Cascade Using the MM Algorithm}{67}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:mm_methods}{{4}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}$L_1$ Estimation for the Inner Problem}{67}}
\newlabel{sec:mm_l1_problem}{{4.1}{67}}
\citation{small1990survey}
\citation{small1990survey}
\citation{boyd2004convex}
\citation{hunter2004tutorial,lange2004optimization,lange2010numerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}An MM Algorithm For Computing the Median}{68}}
\citation{wu2010mm}
\citation{lange2007slides,hunter2004tutorial}
\citation{lange2007slides}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Penalised $L_1$ Fitting}{70}}
\newlabel{sec:mm_l1_algorithm}{{4.1.2}{70}}
\citation{rudin1964principles}
\citation{mclachlan2007algorithm}
\citation{wu2010mm}
\newlabel{eqn:wpensse}{{4.1}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Discussion}{71}}
\newlabel{sec:discuss}{{4.1.3}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Testing the Algorithm on the Melanoma Data}{71}}
\newlabel{sec:testing}{{4.1.4}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison of $L_1$ and $L_2$ inner fits to Cauchy perturbed data with $\omega $ fixed at 0.3\relax }}{73}}
\newlabel{fig:mela_l1_l2}{{4.1}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Plot of values and log differences for SAE Statistic\relax }}{74}}
\newlabel{fig:sae_plot}{{4.2}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Plot of log norm differences for coefficients. Note that they tend to settle on a line.\relax }}{75}}
\newlabel{fig:coef_plot}{{4.3}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Plot of PENSAE statistic as the algorithm proceeds.\relax }}{75}}
\newlabel{fig:pensae_plot}{{4.4}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Two-Stage Parameter Cascade with $L_1$ Norm}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Fitting an $L_1$ Parameter Cascade to the Melanoma Data\relax }}{77}}
\newlabel{fig:mela_l1_cascade}{{4.5}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces $L_1$ and $L_2$ Parameter Cascades with the same perturbed data as in Figure 4.1\hbox {}. Compare the $L_1$ curve in this plot with the one in Figure 4.5\hbox {}.\relax }}{78}}
\newlabel{fig:mela_l1_l2_cascade}{{4.6}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces All possible combinations of $L_1$ and $L_2$ loss functions that can be used for the Parameter Cascade. The top plot applies them to the original melanoma data, the bottom to the same perturbed data as in Figures 4.1\hbox {} and 4.6\hbox {}\relax }}{79}}
\newlabel{fig:mela_l1_l2_combiations}{{4.7}{79}}
\citation{mclachlan2007algorithm}
\citation{mclachlan2007algorithm}
\citation{mclachlan2007algorithm}
\citation{wu2010mm}
\citation{dempster1977maximum,mclachlan2007algorithm}
\citation{isaacson2012analysis}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Accelerating The Rate of Convergence}{80}}
\citation{mclachlan2007algorithm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Illustrative Example: Using Imputation to Fit an ANOVA Model With Missing Data}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Generalisations}{81}}
\newlabel{eqn:rec_seq}{{4.3}{81}}
\citation{isaacson2012analysis}
\citation{chan2017acceleration,dempster1977maximum}
\newlabel{eqn:rec_trans}{{4.4}{82}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Log Errors for original sequence of SSE values and the accelerated one\relax }}{83}}
\newlabel{fig:sse_error}{{4.8}{83}}
\@writefile{toc}{\contentsline {paragraph}{Other Approaches:}{83}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces More sophisticated acceleration methods can provide a further boost to convergence. There are gaps in the plot because the more accelerated iterations have no valid log error since R cannot numerically distinguish them from the final limit.\relax }}{84}}
\newlabel{fig:double_sse_error}{{4.9}{84}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Iterations of the original sequence $SSE_n,$ the accelerated sequence $ASSE_n,$ the quadratically accelerated sequence $QASSE_n,$ and the doubly accelerated sequence $DASSE_n.$\relax }}{85}}
\newlabel{tab:sse_error}{{4.1}{85}}
\citation{graves2000epsilon,osada1993acceleration,wimp1981sequence}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Accelerating the SAE sequence generating by the $L_1$ fitting algorithm using Aitken's Method. The improvement in covergence is mediocre.\relax }}{86}}
\newlabel{fig:sae_accel}{{4.10}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Accelerating the $L_1$ Fitting Algorithm}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Accelerating the SAE sequence using multiple methods. Aitken's method performs the best, despite it's lack of sophistication.\relax }}{87}}
\newlabel{fig:sae_multi_accel}{{4.11}{87}}
\citation{ramsay2017dynamic,hooker2016collocinfer}
\citation{hydon2000symmetry}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Profiling Non-Linear Regression Models With the Parameter Cascade}{88}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Fitting a Non-Linear Regression Model With the Parameter Cascade}{89}}
\newlabel{sec:param_cascade_nonlinear_regress}{{5.1}{89}}
\newlabel{eqn:nonlinear_model}{{5.1}{89}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Fitting Linear Homogeneous ODEs Using the Parameter Cascade}{90}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Profile Plot and Fitted Curve\relax }}{91}}
\newlabel{fig:non_linear_model}{{5.1}{91}}
\newlabel{eqn:ode_model}{{5.2}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Plot of fit to simulated data, and contour plot of SSE against $\alpha $ and $\beta .$ Blue dot is true parameter values, red is estimated parameter values.\relax }}{93}}
\newlabel{fig:ode_plot}{{5.2}{93}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Estimation for First Order Linear PDEs}{94}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}The Transport Equation}{94}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Plot of middle optimisation for the transport equation. Blue line denotes original parameter, red line denotes fitted estimate\relax }}{95}}
\newlabel{fig:transport_profile}{{5.3}{95}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The estimates of $f(x)$ computed for various sample sizes. The plot (a) gives the result with 200 sample points, the plot (b) gives the result for 2000 sample points. To avoid confusion between $x$ and $x - \beta t, z$ was made the independent variable for this plot. Furthermore, the points used to fit $f(x)$ aren't displayed to reduce clutter and make it easier to compare the fitted curves with the original.\relax }}{96}}
\newlabel{fig:transport_equation_fit}{{5.4}{96}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{96}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{96}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Plot of outer optimisation for the modified transport equation with only 30 sample points. Blue line denotes original parameter, red line denotes fitted estimate\relax }}{97}}
\newlabel{fig:mod_transport_profile}{{5.5}{97}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}The Transport Equation with Space-varying Velocity}{97}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Plotted estimate of $f(x)$ for the modified transport equation with 30 sample points.\relax }}{98}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and Further Research}{99}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Derivative-Free versus Derivative-Based Methods}{99}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Further Subjects for Inquiry}{99}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Quasi-linear Differential Problems}{100}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Inviscid Burger's Equation}{100}}
\citation{fisher1937wave}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Discussion}{101}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{102}}
\citation{nocedalnumerical,chong2013introduction}
\citation{nocedalnumerical}
\citation{chong2013introduction}
\citation{nocedalnumerical,chong2013introduction,mathematicsdata2018}
\citation{mathematicsdata2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Overview of Optimisation Methods}{103}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:optimisation_methods}{{A}{103}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Gradient Descent and the Newton-Raphson method}{103}}
\newlabel{sec:newton_gradient}{{A.1}{103}}
\citation{nocedalnumerical,isaacson2012analysis,lange2010numerical,lange2004optimization,chong2013introduction}
\citation{isaacson2012analysis}
\citation{isaacson2012analysis}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Plot of the log errors from applying Gradient Descent to the function $f(x) = x^4.$\relax }}{105}}
\newlabel{fig:grad_descent}{{A.1}{105}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Plot of the log errors from applying Gradient Descent and the Newton-Raphson Method to the function $f(x) = x^4.$\relax }}{106}}
\newlabel{fig:newton_grad_descent}{{A.2}{106}}
\citation{kelley1995iterative}
\citation{kelley1995iterative}
\citation{isaacson2012analysis}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}The Chord Method}{107}}
\newlabel{sec:chord_methods}{{A.2}{107}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}More Sophisticated Hessian Approximations}{107}}
\newlabel{sec:higher_order_methods}{{A.3}{107}}
\citation{nocedalnumerical,lange2004optimization,lange2010numerical}
\citation{nocedalnumerical}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {paragraph}{Linearly Convergent Methods:}{108}}
\@writefile{toc}{\contentsline {paragraph}{Quasi-Newton Methods:}{108}}
\citation{kelley1995iterative}
\citation{rlanguage}
\citation{rlanguage,nocedalnumerical}
\citation{nocedalnumerical}
\citation{nocedalnumerical,leveque2007finite,kelley1995iterative}
\@writefile{toc}{\contentsline {paragraph}{Finite Differences:}{109}}
\citation{kelley1995iterative}
\newlabel{eqn:finite_diff_error}{{A.1}{110}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Plot of the log iterates produced from the recurrence relation $e_{n+1} = \mathaccentV {bar}016{e} + (e_n + h)e_n$ with $e_0 = 0.5,$ for various choices of $\mathaccentV {bar}016{e}$ and $h.$\relax }}{111}}
\newlabel{fig:finite_difference_converge}{{A.3}{111}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Line Search Methods}{111}}
\newlabel{sec:line_search_methods}{{A.4}{111}}
\citation{nocedalnumerical}
\citation{nocedalnumerical}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sketch of Backtracking Line Search\relax }}{112}}
\citation{nocedalnumerical}
\citation{tamir1976line}
\citation{barzilai1982nonpolynomial}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4.1}Wolfe Conditions}{113}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4.2}More sophisticated Line Search Algorithms}{113}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Extrapolating too far out can lead to disaster!\relax }}{114}}
\newlabel{fig:extrapolateDisaster}{{A.4}{114}}
\citation{nocedalnumerical,kelly2002filtering,kelley2011implicit}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Implicit Filtering}{115}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:implicit_filtering}{{B}{115}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Description of the Algorithm}{115}}
\citation{kelley2001implicit}
\citation{nocedalnumerical,kelly2002filtering,kelley2011implicit,kelley2001implicit}
\newlabel{eqn:forward_difference}{{B.1}{116}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Convergence Theory}{116}}
\newlabel{eqn:implicit_filtering_convergence_criterion}{{B.2}{117}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Assessment of the Method}{117}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Using Implicit Filtering to Fit an ODE to the Melanoma Data}{118}}
\newlabel{sec:implicit_brent}{{B.4}{118}}
\newlabel{eqn:quasi_linear_oscillator}{{B.3}{118}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Time taken for Implicit Filtering to fit (B.3\hbox {}) to the melanoma data for various values of $\delta .$\relax }}{118}}
\newlabel{tab:run_times}{{B.1}{118}}
\citation{*}
\bibstyle{plain}
\bibdata{ref}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Fitting the ODE (B.3\hbox {}) to the Melanoma data. The exact value of the shrink value $\delta $ effects the fit the Implicit Filtering algorithm converges to. For $\delta = 0.7$ the computed fit in (a) resembles a straight line, but $\delta = 0.9$ results in a sinusodial plus linear trend as can be seen in (b).\relax }}{119}}
\newlabel{fig:implicit_filtering}{{B.1}{119}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{119}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{119}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Without a penalty term, Implicit Filtering entirely fails to fit the ODE (B.3\hbox {}) to the melanoma data.\relax }}{120}}
\newlabel{fig:implicit_filtering_no_penalty}{{B.2}{120}}
\bibcite{barzilai1982nonpolynomial}{{1}{}{{}}{{}}}
\bibcite{boyd2001chebyshev}{{2}{}{{}}{{}}}
\bibcite{boyd2004convex}{{3}{}{{}}{{}}}
\bibcite{brent2013algorithms}{{4}{}{{}}{{}}}
\bibcite{cao2007parameter}{{5}{}{{}}{{}}}
\bibcite{chan2017acceleration}{{6}{}{{}}{{}}}
\bibcite{chong2013introduction}{{7}{}{{}}{{}}}
\bibcite{dempster1977maximum}{{8}{}{{}}{{}}}
\bibcite{fisher1937wave}{{9}{}{{}}{{}}}
\bibcite{fornberg1988generation}{{10}{}{{}}{{}}}
\bibcite{graves2000epsilon}{{11}{}{{}}{{}}}
\bibcite{hooker2016collocinfer}{{12}{}{{}}{{}}}
\bibcite{hunter2004tutorial}{{13}{}{{}}{{}}}
\bibcite{hydon2000symmetry}{{14}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{121}}
\bibcite{iacus2009simulation}{{15}{}{{}}{{}}}
\bibcite{isaacson2012analysis}{{16}{}{{}}{{}}}
\bibcite{kelley1995iterative}{{17}{}{{}}{{}}}
\bibcite{kelley2011implicit}{{18}{}{{}}{{}}}
\bibcite{kelley2001implicit}{{19}{}{{}}{{}}}
\bibcite{kelly2002filtering}{{20}{}{{}}{{}}}
\bibcite{kiefer1953sequential}{{21}{}{{}}{{}}}
\bibcite{kloeden2013numerical}{{22}{}{{}}{{}}}
\bibcite{lange2004optimization}{{23}{}{{}}{{}}}
\bibcite{lange2010numerical}{{24}{}{{}}{{}}}
\bibcite{lange2007slides}{{25}{}{{}}{{}}}
\bibcite{leveque2007finite}{{26}{}{{}}{{}}}
\bibcite{mcconnell2004code}{{27}{}{{}}{{}}}
\bibcite{mclachlan2007algorithm}{{28}{}{{}}{{}}}
\bibcite{nocedalnumerical}{{29}{}{{}}{{}}}
\bibcite{osada1993acceleration}{{30}{}{{}}{{}}}
\bibcite{pawitan2001all}{{31}{}{{}}{{}}}
\bibcite{rlanguage}{{32}{}{{}}{{}}}
\bibcite{fdapackage}{{33}{}{{}}{{}}}
\bibcite{ramsay2005functional}{{34}{}{{}}{{}}}
\bibcite{data2ld}{{35}{}{{}}{{}}}
\bibcite{ramsay2017dynamic}{{36}{}{{}}{{}}}
\bibcite{ramsay2007parameter}{{37}{}{{}}{{}}}
\bibcite{ramsay2009functional}{{38}{}{{}}{{}}}
\bibcite{rudin1964principles}{{39}{}{{}}{{}}}
\bibcite{schilling2017measures}{{40}{}{{}}{{}}}
\bibcite{schumaker2007spline}{{41}{}{{}}{{}}}
\bibcite{shor2012minimization}{{42}{}{{}}{{}}}
\bibcite{small1990survey}{{43}{}{{}}{{}}}
\bibcite{soetaert2012solving}{{44}{}{{}}{{}}}
\bibcite{desolve}{{45}{}{{}}{{}}}
\bibcite{strauss2007partial}{{46}{}{{}}{{}}}
\bibcite{tamir1976line}{{47}{}{{}}{{}}}
\bibcite{teschl2012ordinary}{{48}{}{{}}{{}}}
\bibcite{vandebogart2017method}{{49}{}{{}}{{}}}
\bibcite{wimp1981sequence}{{50}{}{{}}{{}}}
\bibcite{mathematicsdata2018}{{51}{}{{}}{{}}}
\bibcite{wu2010mm}{{52}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
