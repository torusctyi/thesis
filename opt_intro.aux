\relax 
\citation{nocedalnumerical,chong2013introduction}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Rates Of Convergence}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:rates_of_convergence}{{A}{1}}
\@writefile{toc}{\contentsline {paragraph}{Linear Convergence:}{1}}
\newlabel{eqn:linear_convergence}{{A.1}{1}}
\@writefile{toc}{\contentsline {paragraph}{Sublinear Convergence:}{1}}
\citation{nocedalnumerical}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Illustrating the different classes of convergence.\relax }}{2}}
\@writefile{toc}{\contentsline {paragraph}{Superlinear and Quadratic Convergence:}{2}}
\newlabel{eqn:linear_convergence}{{A.2}{2}}
\@writefile{toc}{\contentsline {paragraph}{An Extended Definition of Convergence Rates:}{2}}
\@writefile{toc}{\contentsline {paragraph}{Linear Convergence and Iterated Mappings:}{2}}
\citation{nocedalnumerical,chong2013introduction}
\citation{nocedalnumerical,chong2013introduction,mathematicsdata2018}
\citation{mathematicsdata2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Overview of Optimisation Methods}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Gradient Descent and the Newton-Raphson Method}{4}}
\newlabel{sec:newton_gradient}{{B.1}{4}}
\citation{nocedalnumerical,isaacson2012analysis,lange2010numerical,lange2004optimization,chong2013introduction}
\citation{isaacson2012analysis}
\citation{isaacson2012analysis}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Plot of the log errors from applying Gradient Descent to the function $f(x) = x^4.$\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:grad_descent}{{B.1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Plot of the log errors from applying Gradient Descent and the Newton-Raphson Method to the function $f(x) = x^4.$\relax }}{7}}
\newlabel{fig:newton_grad_descent}{{B.2}{7}}
\citation{kelley1995iterative}
\citation{kelley1995iterative}
\citation{isaacson2012analysis}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}The Chord Method}{8}}
\newlabel{sec:chord_methods}{{B.2}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}More Sophisticated Hessian Approximations}{8}}
\newlabel{sec:higher_order_methods}{{B.3}{8}}
\citation{nocedalnumerical,lange2004optimization,lange2010numerical}
\citation{nocedalnumerical}
\citation{nocedalnumerical}
\@writefile{toc}{\contentsline {paragraph}{Linearly Convergent Methods:}{9}}
\@writefile{toc}{\contentsline {paragraph}{Quasi-Newton Methods:}{9}}
\citation{kelley1995iterative}
\citation{rlanguage}
\citation{rlanguage,nocedalnumerical}
\citation{nocedalnumerical}
\citation{nocedalnumerical,leveque2007finite,kelley1995iterative}
\@writefile{toc}{\contentsline {paragraph}{Finite Differences:}{10}}
\citation{kelley1995iterative}
\newlabel{eqn:finite_diff_error}{{B.1}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Plot of the log iterates produced from the recurrence relation $e_{n+1} = \mathaccentV {bar}016{e} + (e_n + h)e_n$ with $e_0 = 0.5,$ for various choices of $\mathaccentV {bar}016{e}$ and $h.$\relax }}{12}}
\newlabel{fig:finite_difference_converge}{{B.3}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Line Search Methods}{12}}
\citation{nocedalnumerical}
\citation{nocedalnumerical}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sketch of Backtracking Line Search\relax }}{13}}
\citation{tamir1976line}
\citation{barzilai1982nonpolynomial}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4.1}More sophisticated Line Search Algorithms}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Extrapolating too far out can lead to disaster!\relax }}{15}}
\newlabel{fig:extrapolateDisaster}{{B.4}{15}}
\citation{nocedalnumerical,kelly2002filtering,kelley2011implicit}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Implicit Filtering}{16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Description of the Algorithm:}{16}}
\citation{kelley2001implicit}
\citation{nocedalnumerical,kelly2002filtering,kelley2011implicit,kelley2001implicit}
\newlabel{eqn:forward_difference}{{C.1}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {C.2}Convergence Theory:}{17}}
\newlabel{eqn:implicit_filtering_convergence_criterion}{{C.2}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {C.3}Assessment of the Method:}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {C.4}Using Implicit Filtering to Fit an ODE to the Melanoma Data}{18}}
\newlabel{sec:implicit_brent}{{C.4}{18}}
\newlabel{eqn:quasi_linear_oscillator}{{C.3}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces Time taken for Implicit Filtering to fit (C.3\hbox {}) to the melanoma data for various values of $\delta .$\relax }}{19}}
\newlabel{tab:run_times}{{C.1}{19}}
\citation{*}
\bibstyle{plain}
\bibdata{ref}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Fitting the ODE (C.3\hbox {}) to the Melanoma data. The exact value of the shrink value $\delta $ effects the fit the Implicit Filtering algorithm converges to. For $\delta = 0.7$ the computed fit in (a) resembles a straight line, but $\delta = 0.9$ results in a sinusodial plus linear trend as can be seen in (b).\relax }}{20}}
\newlabel{fig:implicit_filtering}{{C.1}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Without a penalty term, Implicit Filtering entirely fails to fit the ODE (C.3\hbox {}) to the melanoma data.\relax }}{21}}
\newlabel{fig:implicit_filtering_no_penalty}{{C.2}{21}}
\bibcite{barzilai1982nonpolynomial}{{1}{}{{}}{{}}}
\bibcite{boyd2001chebyshev}{{2}{}{{}}{{}}}
\bibcite{boyd2004convex}{{3}{}{{}}{{}}}
\bibcite{brent2013algorithms}{{4}{}{{}}{{}}}
\bibcite{cao2007parameter}{{5}{}{{}}{{}}}
\bibcite{chan2017acceleration}{{6}{}{{}}{{}}}
\bibcite{chong2013introduction}{{7}{}{{}}{{}}}
\bibcite{dempster1977maximum}{{8}{}{{}}{{}}}
\bibcite{fisher1937wave}{{9}{}{{}}{{}}}
\bibcite{fornberg1988generation}{{10}{}{{}}{{}}}
\bibcite{graves2000epsilon}{{11}{}{{}}{{}}}
\bibcite{hunter2004tutorial}{{12}{}{{}}{{}}}
\bibcite{isaacson2012analysis}{{13}{}{{}}{{}}}
\bibcite{kelley1995iterative}{{14}{}{{}}{{}}}
\bibcite{kelley2011implicit}{{15}{}{{}}{{}}}
\bibcite{kelley2001implicit}{{16}{}{{}}{{}}}
\bibcite{kelly2002filtering}{{17}{}{{}}{{}}}
\bibcite{kiefer1953sequential}{{18}{}{{}}{{}}}
\bibcite{lange2004optimization}{{19}{}{{}}{{}}}
\bibcite{lange2010numerical}{{20}{}{{}}{{}}}
\bibcite{lange2007slides}{{21}{}{{}}{{}}}
\bibcite{leveque2007finite}{{22}{}{{}}{{}}}
\bibcite{mcconnell2004code}{{23}{}{{}}{{}}}
\bibcite{mclachlan2007algorithm}{{24}{}{{}}{{}}}
\bibcite{nocedalnumerical}{{25}{}{{}}{{}}}
\bibcite{osada1993acceleration}{{26}{}{{}}{{}}}
\bibcite{pawitan2001all}{{27}{}{{}}{{}}}
\bibcite{rlanguage}{{28}{}{{}}{{}}}
\bibcite{fdapackage}{{29}{}{{}}{{}}}
\bibcite{ramsay2005functional}{{30}{}{{}}{{}}}
\bibcite{ramsay2007parameter}{{31}{}{{}}{{}}}
\bibcite{ramsay2009functional}{{32}{}{{}}{{}}}
\bibcite{rudin1964principles}{{33}{}{{}}{{}}}
\bibcite{schumaker2007spline}{{34}{}{{}}{{}}}
\bibcite{small1990survey}{{35}{}{{}}{{}}}
\bibcite{desolve}{{36}{}{{}}{{}}}
\bibcite{tamir1976line}{{37}{}{{}}{{}}}
\bibcite{teschl2012ordinary}{{38}{}{{}}{{}}}
\bibcite{vandebogart2017method}{{39}{}{{}}{{}}}
\bibcite{wimp1981sequence}{{40}{}{{}}{{}}}
\bibcite{mathematicsdata2018}{{41}{}{{}}{{}}}
\bibcite{wu2010mm}{{42}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
