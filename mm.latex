\chapter{A Two-Stage $L_1$ Parameter Cascade Using the MM Algorithm} \label{chap:mm_methods}

In Chapter \ref{chap:brent}, Brent's method was introduced and was used to tackle Parameter Cascade problems where the middle level uses a loss function which is difficult to differentiate or has no well-defined derivative everywhere. It was remarked that Brent's method ensures that the different elements of the Parameter Cascade are loosely coupled from each other and this allows one to combine different fitting methodologies for different levels straightforwardly. In this chapter, these ideas are developed further. First, the $L_2$ based penalised fitting method is extended to the $L_1$ case. This new method is then used alongside Brent's method to implement a two level Parameter Cascade with $L_1$ loss functions at both levels. 

\section{$L_1$ Estimation for the Inner Problem} \label{sec:mm_l1_problem}

Brent's method is designed to optimise real-valued functions over a real interval. In Section \ref{sec:cauchy_estimation}, when the Cauchy likelihood was optimised, it was extended to functions that take more than one real argument by optimising over each coordinate ($\mu$ and $\sigma$) separately. However, there is no guarantee that this approach will perform well, and it can fail entirely for functions that have an exotic topography or multiple local optima arranged unusually\footnote{ Consider for example the problem of finding the minimum of  the function $f(x) = x \sin(x)$ over the interval $[0,13].$ It is easy to see that the minimum is not on the boundary points of the interval because $f(0) = 0, f(6) = -1.67,$ and $f(13) = 5.45.$ Brent's method fails to find the minimum. It claims the optimal value is given by $f(4.9) = -4.81$ though $f(11) = -10.99.$}. Even in the best case, optimising over each coordinate generates its own optimisation sub-problem, which has the cumulative effect of increasing the running time of the algorithm. Brent's method further requires the specification of a bounding box that contains the optimal point since it uses bisection, which gets increasingly difficult as the number of dimensions increases. All of these considerations mean that Brent's method is highly unsuitable for performing $L_1$ fitting over a space of functions which tend to have a large number of dimensions. For the inner problem in the Parameter Cascade, by definition, there is one dimension introduced for each basis function used to represent $f$. \\ 

\noindent As a result a different approach will be employed for $L_1$ fitting at the inner level of the cascade. This approach a generalisation of the Iteratively Re-weighted Least Squares (IWLS) algorithm for computing the $L_1$ median of a set of $N$ items $\{x_1, \dots, x_N\}$ to which an $L_1$ norm can be associated. The $L_1$ median is defined as  the object $x$ that minimises $\sum_{i=1}^N |x - x_i|.$   We will start by describing how IWLS can be used to compute the $L_1$ median of a set of real numbers. We will further show that this as an example of what is known as an Majorise-Minimise (MM) algorithm \cite{hunter2004tutorial,lange2004optimization,lange2010numerical}, and then proceed to extend this MM algorithm to produce a modified Penalised Sum of Squares problem that can be iteratively solved and re-weighted to find the function that minimises a penalised $L_1$ norm. 

\subsection{An MM Algorithm For Computing the Median}
Suppose that given a set of numbers $\{x_1, \dots, x_N\}$, the number $x$ that minimised the $L_1$ distance between them is required. Then the fitting criterion, the Sum of Absolute Errors (SAE), is:

\[
   SAE(x) = \sum_{i=1}^N |x_i - x|
\]

\noindent It is well known that $SAE(x)$ is minimised by the sample median of the numbers \cite{small1990survey}. The usual approach to computing the sample median - sorting the numbers and taking the one in the middle -  cannot be generalised to FDA problems. Therefore we will use a different approach. The main difficulty is that the function $SAE(x)$ is not everywhere differentiable, which means that the usual derivative-based techniques such as gradient descent or Newton's method can't be used. Instead an approach known as Majorise-Minimise or the MM Algorithm will be used to minimise the SAE \cite{hunter2004tutorial,lange2004optimization,lange2010numerical}.  For a given iterate $x_n$, a function $M(x|x_n)$ is required with the following properties:

\begin{align*}
     M(x|x_n)    &\geq SAE(x) \\
     M(x_n| x_n) &= SAE(x_n)
\end{align*}

\noindent The function $M(x|x_n)$ is said to majorise $SAE(x).$ The next iterate $x_{n+1}$ is then found as the value of $x$ that minimises $M(x|x_n).$ Thus:

\begin{align*}
   SAE(x_{n+1}) &\leq M(x_{n+1}|x_n) \\
                &\leq M(x_n| x_{n+1}) \\
                &= SAE(x_n) \\
\end{align*}

\noindent If such a function, $M(x|y)$, could be determined such that $M(x|y)$ is straightforward to minimise, it is then possible to easily produce a sequence of iterates $x_n$ such that $SAE(x_{n+1}) \leq SAE(x_n)$ for all $n.$ This pattern of monotone improvement in the objective function is similar to the EM Algorithm. In fact, the EM algorithm is a special case of the MM algorithm \cite{wu2010mm} \footnote{When applied to maximisation problems, MM instead stands for Minorise-Maximise. This case is the same except the surrogate function is required to be less than or equal to the objective function and it is maximised on each iteration. Thus, each iteration drives the objective function upwards.}. \\

\noindent The most important question associated with the MM algorithm is the construction of the majorising function. Once the majoriser has been found, the algorithm is easy to implement \cite{lange2007slides, hunter2004tutorial}. Verifying a potential majoriser is usually straightforward, however finding one in the first place is more difficult. The EM algorithm for example takes advantage of the probabilistic structure of the problem and Jensen's inequality. For an $L_1$ problem, the usual approach  is to employ the Arithmetic Mean-Geometric Mean (AM-GM) inequality \cite{lange2007slides}. Only the AM-GM inequality in its simplest form is required here, i.e.\ that the geometric mean of two numbers is less than or equal to their arithmetic mean:

\[
    \sqrt{xy} \leq \frac{x + y}{2}
\]

\noindent The AM-GM inequality is in fact a special case of Jensen's Inequality since the log function is concave:

\begin{align*}
    \log(\frac{x + y}{2}) &\geq \frac{\log{x}}{2} + \frac{\log{y}}{2} \\
                                   &= \log{\sqrt{x}} + \log{\sqrt{y}}\\
                                  &= \log{\sqrt{xy}}
\end{align*}

\noindent It is possible to exploit the AM-GM inequality to majorise an $L_1$ regression problem by a weighted $L_2$ problem. The $L_1$ norm can be represented as a geometric mean, which then allows for the $L_1$ norm to majorised and separated by a weighted sum of squares. Given an iterate $x_n,$ the AM-GM inequality implies that:

\begin{align*}
    |y - x| &= \sqrt{(y - x)^2} \\
            &= \sqrt{\frac{(y-x)^2}{|y - x_n|}|y-x_n|} \\
            &\leq \frac{1}{2}\left(\frac{(y-x)^2}{|y - x_n|} + |y-x_n|\right)
\end{align*}

\noindent This in turn implies that:

\begin{align*}    
     \sum |x_i - x| &\leq  \frac{1}{2}\sum\left(\frac{(x_i-x)^2}{|x_i - x_n|} + |x_i-x_n|\right) \\
                   &=        \frac{1}{2}\sum\frac{(x_i-x)^2}{|x_i - x_n|} +  \frac{1}{2}\sum\left(|x_i-x_n|\right)
\end{align*}

\noindent The $L_1$ problem is thus majorised by a weighted least squares problem. The $\frac{1}{2}\sum|x_i-x_n|$ term is constant with respect to $x,$ so neglecting it makes no difference to the choice of $x$ that is optimal. Likewise, multiplying the weighted least squares problem by a positive constant doesn't change the optimal value, so the $\frac{1}{2}$ term can be eliminated by multiplying by $2$.  The optimal value $x_{n+1}$ can thus be found by minimising this weighted least squares score:

\[ 
   \sum \frac{(x_i-x)^2}{|x_i - x_n|}
\]

\noindent The algorithm consists of finding the value of $x$ that minimises the least squares error, inversely weighted by the residuals from the previous iteration. 

\subsection{Penalised $L_1$ Fitting} \label{sec:mm_l1_algorithm}

For the case of penalised regression, the Penalised Sum of Absolute Errors (PENSAE) is defined by:

\[
  PENSAE(f|\theta, \lambda) = \sum |x_i - f(t_i)| + \lambda \int |Tf|^2 dt
\]

\noindent Here $T$ is used instead of $L$ to denote a differential operator that might not necessarily be linear.As before, this can be majorised by a weighted sum of a residual-weighted penalised sum of squared errors, and a $\sum |x_i - f_n(t_i)|$  term that can be ignored in the course of the optimisation. Then:

\begin{align} \label{eqn:wpensse}
    PENSAE(f) &\leq \frac{1}{2}WPENSSE(f|f_n, \theta, 2\lambda) + \frac{1}{2} \left(\sum |x_i - f_n(t_i)| \right)\\
              &= \frac{1}{2}\left( \sum \frac{[x_i - f(t_i)]^2}{|x_i - f_n(t_i)|} + 2 \lambda \int |Tf|^2 dt\right) +  \frac{1}{2} \left(\sum |x_i - f_n(t_i)|\right).
\end{align}

\noindent To find the function that minimises the penalised $L_1$ error, one repeatedly finds the function that minimises $WPENSSE$ with the previous set of residuals used as inverse weights. This produces a sequence of fitted functions for which the penalised sum of absolute errors is monotonically forced downwards. 

\subsection{Discussion} \label{sec:discuss}

The  sequence of penalised errors $PENSAE(f_n)$ is monotone decreasing and but cannot be less than zero, so it is a bounded monotone sequence. As a result, the Monotone Convergence Theorem for sequences of real numbers (\citep{rudin1964principles}) thus guarantees that a given generated sequence $PENSAE(f_n)$  will always converge to a limit. There are two caveats. First, the sequence might converge to a different point depending on the starting values and there is no guarantee that the sequence will converge to the lowest possible value of $PENSAE.$ Second, there is no guarantee that the underlying sequence of functions will converge, and may just oscillate between several points. For example, the sequence $-1, 1, -1, \dots$ does not converge but the associated sequence of absolute values $1,1,1, \dots$ does. \\


\noindent This approach of associating the objective function with more standard problem that acts as a surrogate is employed in the literature on the EM algorithm. For example, in the introductory chapter of \cite{mclachlan2007algorithm}, the authors discuss how a multinomial estimation problem can be transformed into a binomial problem with missing data by artificially splitting one of the cells. They then construct a simple iterative EM scheme that can then be repeatedly iterated to estimate parameters for the original multinomial. They even remark that the surrogate problems associated with EM algorithms tend to be easy to solve using existing tools in the field. In a similar vein, the $L_1$ problem has been replaced here with a surrogate sequence of weighted $L_2$ problems that can easily solved using the \texttt{FDA} package. Since the \texttt{FDA} package does much of the heavy lifting, the necessary code for implementing penalised $L_1$ regression is brief. \\

\noindent Although the MM algorithm is simple to implement and good at tackling high dimensional penalised regression, convergence can be slow \cite{wu2010mm}. These claims are borne out when the convergence of the method is examined in Section \ref{sec:testing} below.

\subsection{Testing the Algorithm on the Melanoma Data} \label{sec:testing}

Since minimising $PENSAE$ is an optimisation problem over many dimensions, plotting the objective function to verify that the optimal function has been found isn't possible. Instead the MM algorithm described in Section \ref{sec:mm_l1_algorithm} will be tested by applying it to the melanoma data perturbed by random noise. Further, the convergence of the algorithm for the original melanoma dataset will be examined. Figure \ref{fig:mela_l1_l2} presents the $L_1$ and $L_2$ inner level fits to the melanoma data, which has been corrupted by Cauchy distributed noise. The value of $\omega$ was held fixed at a value of $0.3,$ which was chosen as the average of the two different estimates of $\omega$ presented in Figures \ref{fig:mela_omega} and \ref{fig:mela_mad_omega} from the previous chapter. It is apparent from the Figure \ref{fig:mela_l1_l2} that the MM fit is robust against outliers, tends to ignore more deviant points, and even manages to remain similar to the original fit. The $L_2$ fit tends to chase the heavy-tailed noise on the other hand. This is strong evidence that the curve that minimises $PENSAE$ has been found and that the method has been implemented correctly. \\

\noindent Figures \ref{fig:sae_plot} and \ref{fig:pensae_plot} plot the convergence of $SAE$ and $PENSAE$ over the course of the algorithm. Note that the PENSAE statistic doesn't quite converge monotonically as the theoretical analysis predicted. Instead, it fluctuates before settling down to the typical and expected pattern of monotone decline. Upon investigation, it was determined that over the first few iterations, the range of the weights applied to the observations on each iteration that were computed using the residuals from the previous iteration, grew very rapidly. By the fourth iteration, the lowest weight was equal to $1.48,$ and the highest was equal to $4.8 \times 10^6.$ It seems that this rapid and large change produces qualitative changes in behaviour before the algorithm manages to  `burn in'. It is likely that observations with low weights are effectively being censored after a few iterations due to roundoff error. It was found that imposing a minimum threshold for the weights by adding a constant to all the residuals before proceeding to computing the weights smooths out this behaviour, but doesn't eliminate it entirely. \\

\noindent Figure \ref{fig:coef_plot} plots the convergence of the coefficient vectors $\mathbf{c}_n.$ This log-plot suggests that the sequence of fitted coefficient vectors $\mathbf{c}_n$ converges linearly since $\|\mathbf{c}_{n+1} - \mathbf{c}_n\| \approx C\|\mathbf{c}_{n} - \mathbf{c}_{n-1}\|$ as $n \rightarrow \infty.$

\begin{figure}
    \centering
    \includegraphics[height=10cm]{mela_l1_l2_plot.pdf}
    \caption{Comparison of $L_1$ and $L_2$ inner fits to Cauchy-perturbed data with $\omega$ fixed at 0.3}
\label{fig:mela_l1_l2}
\end{figure}


\begin{figure}
\centering
   \includegraphics[height=9cm]{mela_l1_convergence_plot.pdf}
   \includegraphics[height=9cm]{mela_l1_convergence_log_plot.pdf}
   \caption{Plot of values and log differences for SAE Statistic}
   \label{fig:sae_plot}
\end{figure}

\begin{figure}
\centering
   \includegraphics[height=9cm]{mela_pensae_plot.pdf}
   \caption{Plot of PENSAE statistic as the algorithm proceeds.}
   \label{fig:pensae_plot}
\end{figure}

\begin{figure}
\centering
   \includegraphics[height=9cm]{mela_coef_convergence_plot.pdf}
   \caption{Plot of log norm differences for coefficients. Note that they tend to settle on a line.}
    \label{fig:coef_plot}
\end{figure}

\clearpage



\section{The Two-Stage Parameter Cascade with $L_1$ Estimation at Inner and Middle Levels}

The inner problem of the parameter cascade is a semi-parametric least squares regression model. The fitted function is modelled as a weighted sum of a solution to the differential equation (parametric), and a non-parametric residual. The $\lambda$ term governs how big the residual is allowed to be relative to the the least squares error term. If the usual least-squares error function is used, the inner problem will  struggle with outliers and heavy tailed errors as is the case for any form of least-squares regression. For high order differential operators like that used to model the melanoma data, there are  many degrees of freedom associated with the differential operator's solution set. The $\omega$ and $\lambda$ parameters don't strongly constrain the lower level of the cascade. There is thus little capacity for the higher levels of the cascade to restrain the lowest level through altering the $\lambda$ and $\omega$ parameters and the Parameter Cascade algorithm must therefore use robust estimation at every level. \\

\noindent Chapter \ref{chap:brent} discussed how Brent's Method can be used to tackle the middle problem without derivatives and then used this approach to optimise a highly irregular loss function. In Section \ref{sec:mm_l1_problem}, the MM algorithm was used to optimise the inner problem with an $L_1$ norm. Combining the two methods, it is very straightforward to implement a two-level Parameter Cascade with $L_1$ errors at both levels. Figure \ref{fig:mela_l1_cascade} displays the result of fitting a two level $L_1$ Parameter Cascade using the proposed approach to the original melanoma data. It can be seen that the $SAE(\omega)$ function is highly irregularly shaped, so we must be more careful that the optimisation routine doesn't get stuck on a local minimum. In Figure \ref{fig:mela_l1_l2_cascade} both the $L_1$ and $L_2$ fits to the melanoma data perturbed with Cauchy noise are shown, where the same loss function is used respectively for both levels. Figure \ref{fig:mela_l1_l2_combiations} plots the results of applying the $L_1$ and $L_2$ Parameter Cascades to the original and perturbed melanoma data, alongside mixed versions where the $L_1$ loss function is used for the inner fitting and $L_2$ loss function for the middle fitting and vice versa. It can be seen that using the $L_1$ loss function for the inner criterion is more robust than the $L_2$ loss function. The penalty term applied to the inner fitting only has limited capacity to stop the least squares loss function from chasing outliers.



\begin{figure} 
\centering
    \includegraphics[height=9cm]{mela_omega_mae_profile.pdf}
    \includegraphics[height=9cm]{mela_omega_mae_fit.pdf}
    \caption{Fitting an $L_1$ Parameter Cascade to the Melanoma Data}
    \label{fig:mela_l1_cascade}
\end{figure}

\begin{figure} 
\centering
 \includegraphics[height=9cm]{mela_l1_l2_cascade_plot.pdf}
 \caption{$L_1$ and $L_2$ Parameter Cascades with the same perturbed data as in Figure \ref{fig:mela_l1_l2}. Compare the $L_1$ curve in this plot with the one in Figure \ref{fig:mela_l1_cascade}.}
 \label{fig:mela_l1_l2_cascade}
\end{figure}

\newpage

\begin{figure}
\centering
\includegraphics[height=9cm]{mela_l1_l2_combinations.pdf} 
\includegraphics[height=9cm]{mela_l1_l2_combinations_perturbed.pdf}
\caption{All possible combinations of $L_1$ and $L_2$ loss functions that can be used for the Parameter Cascade. The top plot applies them to the original melanoma data, the bottom to the same perturbed data as in Figures \ref{fig:mela_l1_l2} and \ref{fig:mela_l1_l2_cascade}}
\label{fig:mela_l1_l2_combiations} 
\end{figure}

\newpage
\clearpage
\section{Accelerating The Rate of Convergence}

The MM Algorithm is very sluggish, and this is a well known weakness of both itself and the EM algorithm. However \cite{mclachlan2007algorithm} suggests that this problem could be easily ameliorated in this particular case because of a special feature present. In practice, fitting the full model is not required and the computation of an associated summary statistic that determines how good a given choice of parameter is, is all that is needed. For our purposes, it will often be the case that only the value of $PENSSAE$ or $GCV$ or $SAE$ associated with a given choice of parameters is required as inputs to an optimisation routine, and it is not desirable to iterate until all the parameters for the full model converge if this effort can be avoided. In all the situtations in Chapter \ref{chap:brent} where the Melanoma data was fitted for example, we generally didn't care about the coefficient vector computed at the lower level of the optimisation as such, and were only interested in it insofar as it could be employed to compute the loss function for a given parameter choice.  \\

\noindent MacLanan and Krishnan (\cite{mclachlan2007algorithm}) discuss the situation where one only wants to compare the likelihoods between a restricted model and a full model. They suggest the use of sequence acceleration methods to rapidly extract the likelihoods instead of running the EM algorithm to completion since the full models aren't needed. Wu (\cite{wu2010mm}) claims that acceleration methods for the EM algorithm translate quite easily to the MM case. On this basis, we explored whether this approach might be applied here to speed up the proposed estimation procedure. \\

\noindent The approach employed is known as Aitken Acceleration \cite{dempster1977maximum, mclachlan2007algorithm}. Suppose that there is a sequence $x_0, x_1, x_3, \dots$ converging to a limit $x^*$. Aitken's method makes the ansatz that $ x_{n+1} - x^* \approx C(x_n - x^*)$  for some constant $C$. Many iterative algorithms in statistics exhibit this pattern as discussed in Section \ref{sec:discuss}. 
This suggests the following equation:

\[
  \frac{x_{n+1} - x^*}{x_{n} - x^*} \approx \frac{x_{n} - x^*}{x_{n-1} - x^*}
\]

\noindent Solving for $x^*$ gives the accelerated sequence. There is an equivalent definition that is easier to generalise as given in \cite{isaacson2012analysis}. Consider a sequence defined by functional iteration so that $x_{n+1} = F(x_n)$ for some function $F(\cdot).$ Define the error sequence by $e_{n} = x_{n+1}- x_n = F(x_n) - x_n.$ The function $g(x) = F(x) - x$ returns the error associated with any value, and the limit of the sequence satisfies $g(x^*) = 0.$ Suppose that the inverse of $g(x)$ in known and denoted by $h(e).$ Then $x^*$ could be found by evaluating $f(0).$ The next best thing would be to use the values of the sequence to approximate $h(e),$ and then evaluate this approximate function at zero instead. The Aitken method approximates $h(e)$ by linear interpolation between $(e_n, x_n)$ and $(e_{n-1}, x_{n-1}),$ and then evaluates this approximation at $e = 0.$ 

\newpage

\subsection{Example: Using Imputation to Fit an ANOVA Model With Missing Data}

For illustrative purposes, we will make use of an example from Chapter 2 of \cite{mclachlan2007algorithm}. The authors discuss fitting an ANOVA model to a factorial experiment where some of the values are missing. They proceed by using the fitted model to estimate the missing values, fitting the model again with the new imputed values, and using the new fitted values in turn to  update the estimates of missing values. The process is repeated until convergence. In the text, the authors do not work with likelihood or any probabilistic models and treat the question as a regression problem. This is similar to our $L_1$ fitting problem. The authors' example was implemented in R. For each iteration, the $SSE$ statistic was computed. This defines an associated sequence $\{SSE_1, SSE_2,\dots, SSE_n, \dots\}.$ Applying Aitken's method to this sequence produces a new sequence $\{ASSE_n\}.$ As can be seen in Figure \ref{fig:sse_error} and Table \ref{tab:sse_error}, the accelerated sequence converges far more quickly to the limit of the $\{SSE_i\}$ sequence than the original sequence.

\subsection{Generalisations}

Exploring more powerful methods than Aitken's method can be justified in two circumstances. The first is that if one is running the algorithm over and over again such that an increase in speed over many iterations means the effort invested is worth it. This might be the case for example if one wanted to use the bootstrap to model the distribution of  a likelihood ratio statistic computed using the EM algorithm as previously described. The second is if the sequence is difficult to accelerate. In the context of accelerating the MM fitting algorithm introduced in Section \ref{sec:mm_l1_problem}, it shall be seen that both conditions apply. \\

\noindent As a field of study, sequence acceleration is closely related to time series analysis. A generic first order autoregressive model is given by

\[
   x_{n+1} = f(x_n, n) +  \epsilon_n.
\]

\noindent Consider the case where there are  both no random errors so that $\epsilon_n$ is always zero, and the sequence converges to a limit. Here, the problem of determining the long term value of the sequence from a set of observations is equivalent to that of accelerating the sequence. If the specific form of $f(x_n, n)$ is known, there can often be a specific acceleration method that can exactly extract the limit. For  illustration, suppose there were a sequence of the following form, but the parameters $\beta_0$ and $\beta_1$ were unknown:

\begin{equation} 
  x_n = \beta_0 + \frac{\beta_1}{n}. \label{eqn:rec_seq}
\end{equation}

\noindent As $n$ goes to infinity, $x_n$ converges to $\beta_0.$ It is not difficult to show that the limit $\beta_0$ can be found by applying  the following sequence transformation:

\begin{equation}  
   \begin{cases} 
      \begin{aligned}
       \hat{\beta}_{1,n} &= \frac{x_n - x_{n+1}}{\left(\frac{1}{n} - \frac{1}{n+1}\right)} \\
       \tilde{x}_n &= x_n - \frac{\hat{\beta}_{1,n}}{n}.
     \end{aligned}
  \end{cases} \label{eqn:rec_trans}
\end{equation}

\noindent If the transformation in Equation (\ref{eqn:rec_trans}) is applied to a sequence of values $x_1, x_2, \dots, x_n, \dots$ that is of form presented in Equation (\ref{eqn:rec_seq}), then the transformed sequence $\tilde{x}_1, \tilde{x}_2, \dots, \tilde{x}_n, \dots$ will have the property that $\tilde{x}_n = \beta_0$ for all $n.$ Likewise, the Aitken method is exact for sequences of the form $x_{n+1} = \beta_0 + \beta_1x_n,$ and so can be thought of as the deterministic analogue of an $AR(1)$ model. \\

\noindent The process of acceleration isn't quite as neat in practice because sequences don't adhere perfectly to these simple forms. Instead, the best that can be hoped for is that the transformed sequence converges to the same limit as the original one, but the rate of convergence is higher. For example, if transformation (\ref{eqn:rec_trans}) is applied to a sequence of the form $y_n = \beta_0 + \frac{\beta_1}{n} + \frac{\beta_2}{n^2},$ then the transformed sequence is now of the form $\tilde{y}_n = \beta_0 + \mathcal{O}(\frac{1}{n^2}),$ which converges to $\beta_0$ more quickly than the original sequence.\footnote{Doing the algebra, it can be seen that it is now the case that $\hat{\beta}_{1,n} =  \beta_1 + \beta_2 \left[\frac{2n + 1}{n(n+1)}\right],$ and so $\tilde{y}_n = \beta_0 +\beta_{2}\left[-\frac{2n + 1}{n^2(n+1)} + \frac{1}{n^2}\right] = \beta_0  + \beta_{2}\left[\frac{-n^3 - n^2  + n}{n^4(n+1)}\right] .$}
Suppose a convergent  sequence is of the form $x_{n+1} = f(x_{n})$ with $f(\cdot)$ differentiable and $x^*$ is the limit. Using a first order Taylor expansion, it can be seen that for sufficiently large $n,$ $x_{n+1} \approx  x^* +  f'(x^*)(x_n - x^*).$ In this case, Aitken acceleration has a decent chance of accelerating the sequence so long as it has `burned in' sufficiently. \\

\noindent One generalisation, proposed in \cite{isaacson2012analysis} is to use higher order polynomials to model the inverse error function $h(e).$ In this case $h(e)$ would be approximated by a quadratic through $(e_n, x_n), (e_{n-1}, x_{n-1})$ and $(e_{n-2}, x_{n-2}).$ Making $e$ the independent variable here instead of $x$ means the estimated limit can simply be found by evaluating the approximating quadratic at $e = 0$ instead of finding the correct root of a quadratic to compute each element of the accelerated sequence. Another approach is to simply apply Aitken Acceleration to the sequence twice. Both of these generalised approaches were attempted for the missing data model described above, and the results can be seen in Table \ref{tab:sse_error} and Figure \ref{fig:double_sse_error}. It can be seen that both methods improve convergence, though double Aitken acceleration is more effective (and easier to implement). One can take the process further. For the missing values linear model, these higher-order methods converge very rapidly and are prone to numerical instability thereafter due to the error terms being so small. If the Aitken method is applied three times to the original sequence, the first entry yields the limit immediately and there is no need to go any further. Applying the quadratic method twice in a row produces a new sequence for which the first entry is within $10^{-12}$ of the limit.


\paragraph{Other Approaches:} 
There are alternative approaches besides those described here. For example, the EM and MM algorithms generate a sequence of coefficient vectors $\{\mathbf{c}_0, \mathbf{c}_1, \dots, \mathbf{c}_n, \dots \}$ with $\mathbf{c}_{n+1} = \mathbf{F}(\mathbf{c}_n)$ for some function $\mathbf{F}(\cdot).$ In our particular situation, the function $\mathbf{F}(\cdot)$ denotes the operator that takes a coefficient vector and returns the coefficient vector that minimises the associated $WPENSSE$ problem (\ref{eqn:wpensse}). The limit of this sequence - should it exist - is a solution to the equation $\mathbf{c} = \mathbf{F}(\mathbf{c}).$ It is proposed in the literature to use Newton or Quasi-Newton methods as  described in Chapter \ref{chap:brent} and Appendix \ref{chap:optimisation_methods} to numerically solve this fixed point equation \cite{chan2017acceleration, dempster1977maximum}. The idea is that such methods will find the fixed point more rapidly than simply iterating $\mathbf{F}(\cdot)$ until one gets sufficiently close to the limit. These methods have the disadvantage of being more complex and time consuming to implement than the univariate acceleration methods.

\begin{figure}
\centering
   \includegraphics[height=9cm]{anova_accel.pdf}
   \caption{Log Errors for original sequence of SSE values and the accelerated one}
   \label{fig:sse_error}
\end{figure}

\begin{figure}
\centering
   \includegraphics[height=9cm]{anova_double_accel.pdf}
   \caption{More sophisticated acceleration methods can provide a further boost to convergence. There are gaps in the plot because the more accelerated iterations have no valid log error  since  R cannot numerically distinguish them from the final limit.}
   \label{fig:double_sse_error}
\end{figure}
\newpage
\begin{table}[]

\caption{Iterations of the original sequence $SSE_n,$ the accelerated sequence $ASSE_n,$ the quadratically accelerated sequence $QASSE_n,$ and the doubly accelerated sequence $DASSE_n.$}

\label{tab:sse_error}
\centering
\begin{tabular}{ccccc}
     & $SSE_n$          & $ASSE_n$  & $QASSE_n$        & $DASSE_n$       \\
\hline
1     & 4949.6944444444 & 3203.8032711619 & 3203.7834325738 & 3203.7829457122 \\
2     & 3575.1658950617 & 3203.7843303225 & 3203.7829788622 & 3203.7829457359 \\
3     & 3282.7945625667 & 3203.7830400346 & 3203.7829479917 & 3203.7829457364 \\
4     & 3220.5935028609 & 3203.7829521582 & 3203.7829458900 & 3203.7829457364 \\
5     & 3207.3596279977 & 3203.7829461738 & 3203.7829457469 & 3203.7829457364 \\
6     & 3204.5439391293 & 3203.7829457662 & 3203.7829457371 & 3203.7829457364 \\
7     & 3203.9448588925 & 3203.7829457385 & 3203.7829457365 & 3203.7829457364 \\
8     & 3203.8173952920 & 3203.7829457366 & 3203.7829457364 & 3203.7829457364 \\
9     & 3203.7902754193 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
10    & 3203.7845052414 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
11    & 3203.7832775456 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
12    & 3203.7830163340 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
13    & 3203.7829607572 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
14    & 3203.7829489323 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
15    & 3203.7829464164 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
16    & 3203.7829458811 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
17    & 3203.7829457672 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
18    & 3203.7829457430 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
19    & 3203.7829457378 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
20    & 3203.7829457367 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
21    & 3203.7829457365 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
22    & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 & 3203.7829457364 \\
\hline
$\infty$ & 3203.7829457364 & 3203.7829457364 &  3203.7829457364 & 3203.7829457364
\end{tabular}
\end{table}

\newpage
\clearpage
\subsection{Accelerating the $L_1$ Fitting Algorithm}

The $L_1$ fitting algorithm devised in Section \ref{sec:mm_l1_problem} is much more difficult to accelerate as can be seen in Figures \ref{fig:sae_accel} and \ref{fig:sae_multi_accel}. Figure \ref{fig:sae_accel}  shows the result of applying Aitken acceleration to the $SAE$ sequence generated, and Figure \ref{fig:sae_multi_accel} shows the results of attempting numerous other acceleration methods, including approaches specifically designed for accelerating slowly converging sequences that Aitken acceleration cannot accelerate, e.g.\ Epsilon Algorithm and Lubkin's W transform  \cite{graves2000epsilon, osada1993acceleration,wimp1981sequence}. No method performs substantially better than Aitken acceleration. The $SAE$ sequence appears to be either numerically ill-behaved or of a very unusual form. To conclude, it is possible to save some time by acceleration, but the scope for doing so is limited and the process would have to be monitored carefully to ensure that numerical instability isn't an issue. 


\begin{figure}
\centering
   \includegraphics[height=9cm]{l1_accleration_plot.pdf}
   \caption{Accelerating the SAE sequence generating by the $L_1$ fitting algorithm using Aitken's Method. The improvement in covergence is  mediocre.}
\label{fig:sae_accel}
\end{figure}

\begin{figure}
\centering
   \includegraphics[height=9cm]{multi_accleration_plot.pdf}
   \caption{Accelerating the SAE sequence using multiple methods. Aitken's method performs the best, despite it's lack of sophistication.}
\label{fig:sae_multi_accel}
\end{figure}


